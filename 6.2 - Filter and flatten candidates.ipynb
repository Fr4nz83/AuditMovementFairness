{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flatten the list of object IDs representing the candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aux functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_grid_resolution_and_offset_filename(file_candidates : str) -> tuple[int, int] :\n",
    "    tokens = file_candidates.split('_')\n",
    "    \n",
    "    grid_res, grid_offset = tokens[1], tokens[2]\n",
    "    return grid_res, grid_offset\n",
    "\n",
    "\n",
    "def flatten_lists_ids(np_list_candidates : np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    ''' \n",
    "    Flatten the object ID lists associated with the candidates into a 1D array ###\n",
    "    # NOTE: we do this because we can then use joblib's shared memory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    np_list_candidates : np.ndarray\n",
    "        An array of lists, where each list contains the object IDs associated with a candidate.\n",
    "    labels : np.ndarray\n",
    "        A binary array indicating the presence (1) or absence (0) of a certain property for a given set of objects.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    flat_ids : np.ndarray\n",
    "        A 1D array containing all the object IDs associated with the candidates, concatenated together.\n",
    "    indptr : np.ndarray\n",
    "        An array of indices indicating the starting position of each candidate's list in the `flat_ids` array.\n",
    "    lens : np.ndarray\n",
    "        An array containing the length of each candidate's list of associated object IDs.\n",
    "    '''\n",
    "\n",
    "    # Compute the lengths of each candidate's list.\n",
    "    lens = np.fromiter((a.size for a in np_list_candidates),\n",
    "                       dtype=np.int32, count=len(np_list_candidates))\n",
    "\n",
    "    # Compute the starting/ending positions of each candidate's list in the flattened array.\n",
    "    indptr = np.empty(lens.size + 1, dtype=np.uint32)\n",
    "    indptr[0] = 0\n",
    "    np.cumsum(lens, out=indptr[1:])\n",
    "\n",
    "    # Flatten the lists into a single vector.\n",
    "    flat_ids = np.concatenate(np_list_candidates).astype(np.uint32, copy=False)\n",
    "    \n",
    "    return flat_ids, indptr, lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a single numpy vector contaning the candidates of all the grids. The vector will contain the flattened lists of the object IDs associated with the candidates. Also, the notebook generates auxiliary data structures that can be used to pinpoint the exact locations of the cells making up the candidates (recall also that the candidates have been generated from different grids).\n",
    "\n",
    "Optionally, the notebook allows to filter the candidates whose number of associated objects is below a given threshold. This can be very useful to reduce the execution time when doing the Monte Carlo simulations. Note that even low thresholds, e.g., >= 5, greatly reduces the number of candidates to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_num_objects = 1 # Filter out the subset of cells that have associated less than this number of objects.\n",
    "path_candidates = './data_simulator/huge_dataset/gencand/'\n",
    "list_candidates_paths = [f for f in Path(path_candidates).iterdir() if (f.is_file() and (\"flattened\" not in f.name))]\n",
    "\n",
    "# Read the candidates to be tested over a set of grids.\n",
    "np_list_candidates = None\n",
    "list_grids_info = []\n",
    "for path in tqdm(list_candidates_paths, \n",
    "                 desc=\"Processing candidate files\",\n",
    "                 unit=\"file\"):\n",
    "\n",
    "    # Read the candidates that have been generated for a specific grid.\n",
    "    candidates = pd.read_pickle(path)\n",
    "    # print(f\"Reading grid candidates from {path}\")\n",
    "\n",
    "    # Here we filter out the subset of cells that have associated less than 'min_num_objects' objects.\n",
    "    candidates = candidates.loc[candidates['list_users'].apply(len) >= min_num_objects]\n",
    "\n",
    "    # Retrieve from the index the list of candidates generated for this grid.\n",
    "    list_grid_candidates = candidates.index\n",
    "\n",
    "    # Generate two numpy arrays from the candidates DataFrame: one for the list of users associated with each candidate\n",
    "    # (subset of cells), and one for the size (number of cells of a subset) of each candidate.\n",
    "    cand = candidates['list_users'].to_numpy()\n",
    "    np_list_candidates = np.append(np_list_candidates, cand) if np_list_candidates is not None else cand\n",
    "\n",
    "    grid_res, grid_offset = extract_grid_resolution_and_offset_filename(path.stem)\n",
    "    list_grids_info.append((path.name, grid_res, grid_offset, list_grid_candidates, cand.size))\n",
    "    # print(f\"Grid from which the candidates have been computed: {list_grids_info[-1]}\")\n",
    "\n",
    "print(f\"Total number of candidates: {np_list_candidates.size}\")\n",
    "\n",
    "\n",
    "### Flatten the object ID lists associated with the candidates into a 1D array (plus aux arrays) ###\n",
    "flat_ids, indptr, lens = flatten_lists_ids(np_list_candidates)\n",
    "del np_list_candidates  # free memory \n",
    "\n",
    "# Create a dictionary containing all the necessary information needed to reconstruct the non-flattened candidates from all the grids. \n",
    "dict_flattened_candidates = {'flat_ids' : flat_ids,\n",
    "                             'start_pos' : indptr, \n",
    "                             'lengths' : lens,\n",
    "                             'grid_info' : list_grids_info}\n",
    "\n",
    "# Save the dictionary to disk.\n",
    "out_path = Path(path_candidates + \"dict_flattened_candidates.pkl\")\n",
    "with out_path.open(\"wb\") as f:\n",
    "    pickle.dump(dict_flattened_candidates, f)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
