{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing for Spatial Fairness\n",
    "\n",
    "This notebook runs the experiments. The methods are implemented in the `src/function.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "                \n",
    "from functions import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Select the dataset\n",
    "\n",
    "You can select one of the following:\n",
    "- LAR (`data/LAR.csv`) contains the modified Loan/Application Register records in the US for Bank of America for the year 2021; the dataset is created by `data/LAR/create_LAR.ipynb`.\n",
    "- Crime (`data/Crime.csv`) contains predictions about crime incidents in the city of Los Angeles from 2010â€“2019; the predictive model is a Random Forest Classifier and the dataset is created by `data/Crime/create_Crime.ipynb`.\n",
    "- Synth_fair/Synth_unfair/Semisynth (`/data/Synth_fair.csv`, `/data/Synth_unfair.csv`, `/data/Semisynth.csv`) are synthetic datasets create by the notebooks in `data/Synth/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the LAR dataset\n",
    "df = load_data('./data/LAR.csv')\n",
    "label = 'action_taken'\n",
    "## df = filterbbox(df, -87.634938, 24.523096, -80.031362, 31.000888) ## florida\n",
    "## df = filterbbox(df, -80.8736, 25.13742, -80.06279, 25.979434) # miami\n",
    "\n",
    "\n",
    "## load the CRIME_serious dataset\n",
    "# df = load_data('./data/Crime.csv')\n",
    "# label = 'pred'\n",
    "\n",
    "\n",
    "# load a synthetic dataset\n",
    "# df = load_data('./data/Synth_fair.csv') ## FAIR\n",
    "# label = 'label'\n",
    "\n",
    "\n",
    "# df = load_data('./data/Synth_unfair.csv') ## UNFAIR\n",
    "# label = 'label'\n",
    "\n",
    "\n",
    "# df = load_data('./data/Semisynth.csv') ## FAIR\n",
    "# label = 'label'\n",
    "\n",
    "\n",
    "N, P = get_stats(df, label)\n",
    "\n",
    "print(f'N={N} points')\n",
    "print(f'P={P} positives')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtree = create_rtree(df)\n",
    "# rtree = create_rtree_v2(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lat_max = df['lat'].values.max()\n",
    "# lat_min = df['lat'].values.min()\n",
    "# lon_max = df['lon'].values.max()\n",
    "# lon_min = df['lon'].values.min()\n",
    "\n",
    "# mapit = folium.Map(location=[37.09, -95.71], zoom_start=5, tiles=\"Stamen Toner\")\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     if row[label] == 1:\n",
    "#         folium.CircleMarker( location=(row['lat'], row['lon']), color='#00FF00', fill_color='#00FF00', fill=True, opacity=0.4, fill_opacity=0.4, radius=2 ).add_to( mapit )\n",
    "#     elif row[label] == 0:\n",
    "#         folium.CircleMarker( location=(row['lat'], row['lon']), color='#FF0000', fill_color='#FF0000', fill=True, opacity=0.4, fill_opacity=0.4, radius=2 ).add_to( mapit )\n",
    "\n",
    "\n",
    "# mapit.fit_bounds([(lat_min, lon_min), (lat_max, lon_max)])\n",
    "\n",
    "# mapit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Run Experiments\n",
    "\n",
    "There are three experiments:\n",
    "- Unrestricted regions: runs **our approach** on unrestricted regions.\n",
    "- One Partitioning: runs **our approach** against **MeanVar** on regions from a single partitioning.\n",
    "- Multiple Partitionings: runs **MeanVar** on multiple partitionings.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unrestricted regions (corresponds to Sec. 4.3 of the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a set of points (locations), cluster them and then consider the clusters' centroids. Then, for each\n",
    "# centroid find the nearest location in the r-tree: this point will be used as a seed .\n",
    "seeds = create_seeds(df, rtree, 100)\n",
    "print(len(seeds), seeds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a set of seed point IDs and a list of radii, this function creates candidate regions. \n",
    "# For each seed and radius, it queries the spatial index (using query_range) to get all points within a square centered around\n",
    "# the seed and with side 2*radius. This square corresponds to a region, and packages the information into a dictionary.\n",
    "# Purpose: To generate many regions whose fairness (or lack thereof) can be audited.\n",
    "\n",
    "radii = np.arange(0.05, 1.01, 0.05)\n",
    "regions = create_regions(df, rtree, seeds, radii)\n",
    "\n",
    "print(len(regions), 'regions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapit = folium.Map(location=[37.09, -95.71], zoom_start=5, prefer_canvas = True, tiles='cartodbpositron')\n",
    "\n",
    "# Plot the seeds on a map.\n",
    "for point in seeds:\n",
    "    # NOTE: id2loc retrieve from 'df' a specific location according to its ID in 'point'.\n",
    "    folium.CircleMarker(location=id2loc(df, point), color='#0000FF', fill_color='#0000FF', fill=True, opacity=0.4, fill_opacity=0.4, radius=2 ).add_to( mapit )\n",
    "\n",
    "\n",
    "# NOTE: The code below plots two rectangles in the ocean, and has been probably used just for debugging purposes.\n",
    "\n",
    "#center = np.array([26, -126])\n",
    "#r = radii[0]\n",
    "#lower_left = center - r\n",
    "#upper_right = center + r\n",
    "#folium.Rectangle([lower_left, upper_right], color='#F1CF3B').add_to( mapit )\n",
    "\n",
    "#r = radii[-1]\n",
    "#lower_left = center - r\n",
    "#upper_right = center + r\n",
    "#folium.Rectangle([lower_left, upper_right], color='#F1CF3B').add_to( mapit )\n",
    "\n",
    "\n",
    "mapit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = 'both'\n",
    "# direction = 'less_in'\n",
    "# direction = 'less_out'\n",
    "\n",
    "# Function below used to manage the LAR dataset, in which we remap the label \"3\" to \"0\".\n",
    "true_types = get_true_types(df, label)\n",
    "print(true_types[:30])\n",
    "\n",
    "best_region, max_likeli, statistics = scan_regions(regions, true_types, N, P, direction=direction, verbose=True)\n",
    "\n",
    "# statistics.sort(key=lambda x: -x)\n",
    "# print(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## determine the significance threshold based on a desired signif_level\n",
    "\n",
    "n_alt_worlds = 200\n",
    "signif_level = 0.005\n",
    "\n",
    "signif_thresh = get_signif_threshold(signif_level, n_alt_worlds, regions, N, P)\n",
    "print(signif_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## identify regions with test statistic above statistical significance threshold\n",
    "\n",
    "# Sort the max likelihood ratios computed when considering the real data.\n",
    "sorted_statistics = np.sort(statistics)\n",
    "\n",
    "# Now use the threshold computed from the simulations, to find out the number of regions, 'top_k',\n",
    "# for which the null hypotesis is false (according to the given \\alpha = signif_thresh)\n",
    "# when considering max likelihood ration computed from the real data. In other words,\n",
    "# we are determining the number of regions for which the considered classifier *is not spatially fair*.\n",
    "top_k = len(statistics) - np.searchsorted(sorted_statistics, signif_thresh)\n",
    "print(top_k, 'significant regions')\n",
    "\n",
    "# Compute the list of indices that sort the array 'statistics'.\n",
    "# In essence, 'np.argsort' perform an indirect sort along the given axis; it returns an array of indices of the same shape\n",
    "# as the array being sorted that index data along the given axis in sorted order.\n",
    "indexes = np.argsort(statistics)[::-1][:top_k]\n",
    "\n",
    "# Select the 'top_k' regions that are not spatially fair\n",
    "significant_regions = [ regions[i] for i in indexes ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersects(regionA, regionB):\n",
    "    cA = np.array(id2loc(df, regionA['center']))\n",
    "    cB = np.array(id2loc(df, regionB['center']))\n",
    "    rA = regionA['radius']\n",
    "    rB = regionB['radius']\n",
    "\n",
    "    A_top_right = cA + np.array([rA, rA])\n",
    "    A_bottom_left = cA - np.array([rA, rA])\n",
    "    B_top_right = cB + np.array([rB, rB])\n",
    "    B_bottom_left = cB - np.array([rB, rB])\n",
    "\n",
    "    # print(A_bottom_left, A_top_right, B_bottom_left, B_top_right)\n",
    "\n",
    "    return not (A_top_right[0] < B_bottom_left[0] or A_bottom_left[0] > B_top_right[0] or A_top_right[1] < B_bottom_left[1] or A_bottom_left[1] > B_top_right[1])\n",
    "\n",
    "\n",
    "\n",
    "non_olap_regions = []\n",
    "centers = []\n",
    "for region in significant_regions:\n",
    "    center = region['center']\n",
    "    if center in centers:\n",
    "        continue\n",
    "    \n",
    "    no_intersections = True\n",
    "    for other in non_olap_regions:\n",
    "        if intersects(region, other):\n",
    "            no_intersections = False\n",
    "            break\n",
    "    if no_intersections:\n",
    "        centers.append(center)\n",
    "        non_olap_regions.append(region)\n",
    "    # print(region['radius'])\n",
    "\n",
    "print(len(non_olap_regions), 'non-overlapping regions')\n",
    "\n",
    "# over(non_olap_regions[0], non_olap_regions[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find smallest, largest regions\n",
    "\n",
    "min_radius = np.inf\n",
    "max_radius = -np.inf\n",
    "for region in non_olap_regions:\n",
    "    if region['radius'] < min_radius:\n",
    "        min_radius = region['radius']\n",
    "        # region_min_radius = region\n",
    "    if region['radius'] > max_radius:\n",
    "        max_radius = region['radius']\n",
    "        # region_max_radius = region\n",
    "\n",
    "min_points = np.inf\n",
    "max_points = -np.inf\n",
    "for region in non_olap_regions:\n",
    "    if region['radius'] == min_radius and len(region['points']) < min_points:\n",
    "        min_points = len(region['points'])\n",
    "        region_min_radius = region\n",
    "    if region['radius'] == max_radius and len(region['points']) > max_points:\n",
    "        max_points = len(region['points'])\n",
    "        region_max_radius = region\n",
    "\n",
    "print(len(region_min_radius['points']), len(region_max_radius['points']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_circular_regions(df, true_types, non_olap_regions[:5])\n",
    "\n",
    "# show_circular_regions(df, true_types, [region_min_radius, region_max_radius])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Partitioning \n",
    "\n",
    "**NOTE***: In the code seen so far, the authors partitioned the points in space by clustering them. So, no grid was used.\n",
    "Below, they take a more common approach seen in other works, i.e., superimpose a uniform grid over the space, which effectively partitions the points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_max = df['lat'].values.max()\n",
    "lat_min = df['lat'].values.min()\n",
    "lon_max = df['lon'].values.max()\n",
    "lon_min = df['lon'].values.min()\n",
    "print(lat_min, lat_max, lon_min, lon_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create the partitioning (grid) and its partitions (regions)\n",
    "\n",
    "# lat_n = 12 ## number of partitions along vertical axis (latitude)  ## was 12\n",
    "# lon_n = 25 ## number of partitions along horizontal axis (longitude) ## was 25\n",
    "\n",
    "lat_n = 20\n",
    "lon_n = 20\n",
    "\n",
    "\n",
    "grid_info, grid_loc2_idx, regions = create_partitioning(df, rtree, lon_min, lon_max, lat_min, lat_max, lon_n, lat_n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_region, max_likeli, statistics = scan_regions(regions, true_types, N, P, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## determine the significance threshold based on a desired signif_level\n",
    "\n",
    "n_alt_worlds = 1000\n",
    "signif_level = 0.005\n",
    "\n",
    "signif_thresh = get_signif_threshold(signif_level, n_alt_worlds, regions, N, P)\n",
    "print(signif_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## identify regions with statistic above statistical significance threshold\n",
    "\n",
    "sorted_statistics = np.sort(statistics)\n",
    "# print(sorted_statistics[::-1][40:60])\n",
    "# print(np.sort(statistics)[::-1][40:60])\n",
    "\n",
    "top_k = len(statistics) - np.searchsorted(sorted_statistics, signif_thresh)\n",
    "\n",
    "print(top_k, 'significant regions')\n",
    "\n",
    "\n",
    "indexes = np.argsort(statistics)[::-1][:top_k]\n",
    "\n",
    "significant_regions = [ regions[i] for i in indexes ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_grid_region(df, grid_info, true_types, best_region)\n",
    "show_grid_regions(df, grid_info, true_types, significant_regions[:])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeanVar Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## partioning-based scan\n",
    "\n",
    "the_region, max_score, scores = scan_partitioning(regions, true_types)\n",
    "\n",
    "print('max_score', max_score, 'with', len(the_region['points']), 'points')\n",
    "\n",
    "\n",
    "## get the top_k regions\n",
    "\n",
    "top_k = 5\n",
    "\n",
    "ma = np.ma.masked_array(scores, mask=np.isnan(scores))\n",
    "\n",
    "print(-np.sort(-ma)[:top_k])\n",
    "\n",
    "indexes = np.argsort(-ma)[:top_k]\n",
    "\n",
    "# print(indexes)\n",
    "\n",
    "top_regions = [ regions[i] for i in indexes ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_grid_region(df, grid_info, true_types, the_region)\n",
    "show_grid_regions(df, grid_info, true_types, top_regions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## best_region vs the_region\n",
    "\n",
    "the_region = top_regions[0]\n",
    "\n",
    "best_idx = grid_loc2_idx[best_region['grid_loc']]\n",
    "the_idx = grid_loc2_idx[the_region['grid_loc']]\n",
    "\n",
    "print(best_region['grid_loc'], the_region['grid_loc'])\n",
    "print(best_idx, the_idx)\n",
    "\n",
    "print(statistics[best_idx], statistics[the_idx])\n",
    "print(scores[best_idx], scores[the_idx])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Partitionings (does not apply to the method proposed in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_max = df['lat'].values.max()\n",
    "lat_min = df['lat'].values.min()\n",
    "lon_max = df['lon'].values.max()\n",
    "lon_min = df['lon'].values.min()\n",
    "print(lat_min, lat_max, lon_min, lon_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_n_range = (10, 40)\n",
    "lon_n_range = (10, 40)\n",
    "\n",
    "n_partitionings = 100\n",
    "\n",
    "partitionings = []\n",
    "\n",
    "for i in range(n_partitionings):\n",
    "    lat_n = random.randint(*lat_n_range)\n",
    "    lon_n = random.randint(*lon_n_range)\n",
    "\n",
    "    grid_info, grid_loc2_idx, regions = create_partitioning(df, rtree, lon_min, lon_max, lat_min, lat_max, lon_n, lat_n)\n",
    "\n",
    "    partitionings.append((grid_info, grid_loc2_idx, regions))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MeanVar Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_scores = []\n",
    "max_scores = []\n",
    "for partitioning in partitionings:\n",
    "    the_region, max_score, scores = scan_partitioning(partitioning[2], true_types)\n",
    "    mean_score = np.nanmean(scores)\n",
    "    mean_scores.append(mean_score)\n",
    "    \n",
    "    max_scores.append(max_score)\n",
    "\n",
    "    # print(f'{mean_score=:.4f}, {max_score=:.4f}')\n",
    "\n",
    "print(f'mean of means={np.mean(mean_scores):.4f}')\n",
    "print(f'max of maxs={np.max(max_scores):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
