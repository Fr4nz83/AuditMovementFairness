{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing a Classifier for Fairness Based on Movement Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_in_out_probs(labels: np.ndarray, list_users: np.ndarray):\n",
    "    \"\"\"\n",
    "    list_users: object array of 1D int arrays (your candidates[\"list_users\"].to_numpy()).\n",
    "    Returns: inside_mean, outside_mean as float arrays (len = n_candidates)\n",
    "    \"\"\"\n",
    "    \n",
    "    n = labels.size\n",
    "    tot_sum = labels.sum(dtype=np.uint32)\n",
    "\n",
    "    # Compute the number of cells in each candidate subset of cells.\n",
    "    lens = np.fromiter((a.size for a in list_users), dtype=np.uint32, count=len(list_users))\n",
    "\n",
    "    # Exception: if any empty candidate exists, avoid division by zero\n",
    "    if np.any(lens == 0) or np.any(lens == n):\n",
    "        raise ValueError(\"Found candidate with size 0 or size n; handle these cases explicitly.\")\n",
    "\n",
    "    # \"indptr\" = starting offsets of each candidate inside the flattened array\n",
    "    indptr = np.empty(len(lens) + 1, dtype=np.uint32)\n",
    "    indptr[0] = 0\n",
    "    np.cumsum(lens, out=indptr[1:])\n",
    "\n",
    "    # flatten all selected ids\n",
    "    flat_ids = np.concatenate(list_users)\n",
    "\n",
    "    # gather labels for all ids, then sum per candidate via segmented reduction\n",
    "    flat_vals = labels[flat_ids]\n",
    "    inside_sum = np.add.reduceat(flat_vals, indptr[:-1])\n",
    "\n",
    "    # Compute the positive rates inside and outside each candidate.\n",
    "    inside_mean = inside_sum / lens\n",
    "    outside_mean = (tot_sum - inside_sum) / (n - lens)\n",
    "\n",
    "    return inside_mean.astype(np.float32), outside_mean.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a dummy vector of labels 0/1\n",
    "n_objects = 100000\n",
    "positive_rate = 0.6\n",
    "labels = np.random.binomial(n=1, p=positive_rate, size=n_objects).astype(np.int8)\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the candidates to be tested over a certain grid.\n",
    "path_candidates = './data_simulator/huge_dataset/gencand/candidates_100_0.pkl'\n",
    "candidates = pd.read_pickle(path_candidates)\n",
    "candidates['num_users'] = candidates['list_users'].apply(len).astype(np.uint32) # Take note of the number of users\n",
    "\n",
    "# Remove the original index, which contained in the form of tuples the cells making up a candidate subset of cells,\n",
    "# and we do not need them at this step of our approach. Saves a lot of memory.\n",
    "candidates.reset_index(drop=True, inplace=True)\n",
    "# candidates\n",
    "\n",
    "# For the objects associated with each subset of cells, compute the inside and outside positive rates.\n",
    "candidates[\"in_rate\"], candidates[\"out_rate\"] = batch_in_out_probs(labels, candidates[\"list_users\"].to_numpy())\n",
    "\n",
    "del candidates['list_users']\n",
    "candidates.info(memory_usage=\"deep\")\n",
    "# candidates[450:470]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEBUG: Simple for loop-based version of the computation of inside/outside positive rates; to be used for debugging purposes ###"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Simple for loop-based version ###\n",
    "\n",
    "def sequential_compute_in_out_probs(labels: np.ndarray, sel_ids: np.ndarray, \n",
    "                                    tot_num_els : int, tot_sum_labels : float) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    probs[i] = probability of object i\n",
    "    ids = iterable of object IDs (indices)\n",
    "\n",
    "    Returns:\n",
    "        (mean_inside, mean_outside)\n",
    "    \"\"\"\n",
    "\n",
    "    k = sel_ids.size\n",
    "    sum_inside = labels.take(sel_ids).sum(dtype=np.uint32)\n",
    "    mean_inside = float(sum_inside) / k\n",
    "    mean_outside = float(tot_sum_labels - sum_inside) / (tot_num_els - k)\n",
    "    \n",
    "    return mean_inside, mean_outside\n",
    "\n",
    "\n",
    "# Main code\n",
    "sum_all_labels = labels.sum()\n",
    "tot_num_labels = len(labels)\n",
    "list_mean_inside = []\n",
    "list_mean_outside = []\n",
    "for el in tqdm(candidates[\"list_users\"], total=len(candidates[\"list_users\"]), desc=\"Computing in/out probs\"):\n",
    "    # print(f\"Selected IDs: {sel_ids}\")\n",
    "    mean_inside, mean_outside = sequential_compute_in_out_probs(labels, el, tot_num_labels, sum_all_labels)\n",
    "    list_mean_inside.append(mean_inside)\n",
    "    list_mean_outside.append(mean_outside)\n",
    "    # print(f\"Positive rate inside: {mean_inside}, Positive rate outside: {mean_outside}\")\n",
    "\n",
    "candidates[\"in_rate_debug\"], candidates[\"out_rate_debug\"] = list_mean_inside, list_mean_outside"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
