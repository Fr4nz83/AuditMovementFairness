{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing a Classifier for Fairness Based on Movement Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aux functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_lists_ids(np_list_candidates : np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    ''' \n",
    "    Flatten the object ID lists associated with the candidates into a 1D array ###\n",
    "    # NOTE: we do this because we can then use joblib's shared memory.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    np_list_candidates : np.ndarray\n",
    "        An array of lists, where each list contains the object IDs associated with a candidate.\n",
    "    labels : np.ndarray\n",
    "        A binary array indicating the presence (1) or absence (0) of a certain property for a given set of objects.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    flat_ids : np.ndarray\n",
    "        A 1D array containing all the object IDs associated with the candidates, concatenated together.\n",
    "    indptr : np.ndarray\n",
    "        An array of indices indicating the starting position of each candidate's list in the `flat_ids` array.\n",
    "    lens : np.ndarray\n",
    "        An array containing the length of each candidate's list of associated object IDs.\n",
    "    '''\n",
    "\n",
    "    # Compute the lengths of each candidate's list.\n",
    "    lens = np.fromiter((a.size for a in np_list_candidates),\n",
    "                       dtype=np.int32, count=len(np_list_candidates))\n",
    "\n",
    "    # Compute the starting/ending positions of each candidate's list in the flattened array.\n",
    "    indptr = np.empty(lens.size + 1, dtype=np.uint32)\n",
    "    indptr[0] = 0\n",
    "    np.cumsum(lens, out=indptr[1:])\n",
    "\n",
    "    # Flatten the lists into a single vector.\n",
    "    flat_ids = np.concatenate(np_list_candidates).astype(np.uint32, copy=False)\n",
    "    \n",
    "    return flat_ids, indptr, lens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_in_out_probs(labels_objects: np.ndarray, \n",
    "                       flat_ids: np.ndarray, indptr: np.ndarray, lens: np.ndarray,\n",
    "                       tot_sum_labels: int) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    list_users: object array of 1D int arrays (your candidates[\"list_users\"].to_numpy()).\n",
    "    Returns: inside_mean, outside_mean as float arrays (len = n_candidates)\n",
    "    \"\"\"\n",
    "    \n",
    "    # Total number of objects.\n",
    "    num_objects = labels_objects.size\n",
    "\n",
    "    # Gather labels for all ids, then sum per candidate via segmented reduction\n",
    "    flat_vals = labels_objects[flat_ids]\n",
    "    inside_sum = np.add.reduceat(flat_vals, indptr[:-1]).astype(np.float32, copy=False)\n",
    "\n",
    "    # Compute the positive rates inside and outside each candidate.\n",
    "    inside_mean = inside_sum / lens\n",
    "    outside_mean = (tot_sum_labels - inside_sum) / (num_objects - lens)\n",
    "\n",
    "    return inside_mean, outside_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main code\n",
    "\n",
    "Read the dataset with the objects' labels.\n",
    "**TODO**: we are using dummy labels for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset containing the true labels of the objects.\n",
    "n_objects = 100000\n",
    "positive_rate = 0.6\n",
    "labels = np.random.binomial(n=1, p=positive_rate, size=n_objects).astype(np.int8)\n",
    "# labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a single numpy vector contaning the candidates of all the grids. Then, flatten the lists in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_candidates = './data_simulator/huge_dataset/gencand/'\n",
    "list_candidates_paths = [f for f in Path(path_candidates).iterdir() if f.is_file()]\n",
    "\n",
    "# Read the candidates to be tested over a set of grids.\n",
    "np_list_candidates = None\n",
    "for path_candidates in tqdm(list_candidates_paths, \n",
    "                            desc=\"Processing candidate files\",\n",
    "                            unit=\"file\"):\n",
    "\n",
    "    # Read the candidates that have been generated for a specific grid.\n",
    "    candidates = pd.read_pickle(path_candidates)\n",
    "    # print(f\"Reading grid candidates from {path_candidates}\")\n",
    "\n",
    "    # Generate two numpy arrays from the candidates DataFrame: one for the list of users associated with each candidate\n",
    "    # (subset of cells), and one for the size (number of cells of a subset) of each candidate.\n",
    "    cand = candidates['list_users'].to_numpy()\n",
    "    np_list_candidates = np.append(np_list_candidates, cand) if np_list_candidates is not None else cand\n",
    "\n",
    "    # print(f\"Number of candidates: {cand.size}\")\n",
    "\n",
    "print(f\"Total number of candidates: {np_list_candidates.size}\")\n",
    "\n",
    "\n",
    "### Flatten the object ID lists associated with the candidates into a 1D array (plus aux arrays) ###\n",
    "flat_ids, indptr, lens = flatten_lists_ids(np_list_candidates)\n",
    "del np_list_candidates  # free memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we perform the Monte Carlo simulations needed to determine the distribution of the test statistics under the assumption that the null hypothesis is true.\n",
    " \n",
    "The test statistics used is the maximum likelihood ratio computed across the regions of all the grids, while the likelihood function is the binomial-based one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_simulations = 200\n",
    "tot_sum_labels = labels.sum(dtype=np.uint32) # Constant across permutations\n",
    "max_likelihood_ratios_vec = np.empty(num_simulations, dtype=np.float32)\n",
    "for i in tqdm(range(num_simulations)):    \n",
    "    # Shuffle the original labels assigned to the objects. This represents the null hypotesis H_0, according to which\n",
    "    # there is a single global distribution that governs the labels, i.e., there is not one or more sets of geographical regions\n",
    "    # in which the associated objects have an average positive rate that is significantly different than that of the other objects. \n",
    "    rng = np.random.default_rng(i)\n",
    "    shuffled_labels = rng.permutation(labels)\n",
    "\n",
    "    # For the objects associated with each subset of cells, compute their positive rate vs that of the other objects.\n",
    "    np_list_candidates_inrate, np_list_candidates_outrate = batch_in_out_probs(shuffled_labels,\n",
    "                                                                               flat_ids, indptr, lens,\n",
    "                                                                               tot_sum_labels)\n",
    "\n",
    "    # For each candidate, compute the likelihood ratio.\n",
    "\n",
    "    # Determine the maximum ratio found, and append it to a list.\n",
    "\n",
    "\n",
    "# DEBUG\n",
    "# np_list_candidates_inrate, np_list_candidates_outrate"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "from joblib import Parallel, delayed\n",
    "\n",
    "# ---- worker: one permutation -> one max LLR ----\n",
    "def one_simulation(i, labels, flat_ids, indptr, lens, tot_sum_labels):\n",
    "    rng = np.random.default_rng(i)\n",
    "    shuffled_labels = rng.permutation(labels)\n",
    "\n",
    "    inrate, outrate = batch_in_out_probs(\n",
    "        shuffled_labels,\n",
    "        flat_ids, indptr, lens,\n",
    "        tot_sum_labels,\n",
    "    )\n",
    "\n",
    "    # --- your likelihood ratio computation goes here ---\n",
    "    # llr = ...\n",
    "    # return float(np.max(llr))\n",
    "\n",
    "    # Placeholder to keep the example runnable:\n",
    "    return float(np.max(inrate - outrate))\n",
    "\n",
    "\n",
    "num_simulations = 200\n",
    "tot_sum_labels = labels.sum(dtype=np.uint32)\n",
    "\n",
    "# Force memmapping so arrays are shared via mmap even if < 1MB:\n",
    "# - backend=\"loky\" => true process parallelism\n",
    "# - mmap_mode=\"r\"  => read-only sharing\n",
    "# - max_nbytes=1   => trigger memmap for any ndarray > 1 byte (simple + explicit)\n",
    "#\n",
    "# Note: by default, joblib typically memmaps only when arrays are > ~1MB. :contentReference[oaicite:1]{index=1}\n",
    "results = Parallel(\n",
    "    n_jobs=6,\n",
    "    backend=\"loky\",\n",
    "    prefer=\"processes\",\n",
    "    mmap_mode=\"r\",\n",
    "    max_nbytes=1,\n",
    "    verbose=10\n",
    ")(\n",
    "    delayed(one_simulation)(i, labels, flat_ids, indptr, lens, tot_sum_labels)\n",
    "    for i in range(num_simulations)\n",
    ")\n",
    "\n",
    "max_likelihood_ratios_vec = np.asarray(results, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEBUG: Simple for loop-based version of the computation of inside/outside positive rates; to be used for debugging purposes ###"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Simple for loop-based version ###\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def sequential_compute_in_out_probs(labels: np.ndarray, sel_ids: np.ndarray, \n",
    "                                    tot_num_els : int, tot_sum_labels : float) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    probs[i] = probability of object i\n",
    "    ids = iterable of object IDs (indices)\n",
    "\n",
    "    Returns:\n",
    "        (mean_inside, mean_outside)\n",
    "    \"\"\"\n",
    "\n",
    "    k = sel_ids.size\n",
    "    sum_inside = labels.take(sel_ids).sum(dtype=np.uint32)\n",
    "    mean_inside = float(sum_inside) / k\n",
    "    mean_outside = float(tot_sum_labels - sum_inside) / (tot_num_els - k)\n",
    "    \n",
    "    return mean_inside, mean_outside\n",
    "\n",
    "\n",
    "# Main code\n",
    "sum_all_labels = labels.sum()\n",
    "tot_num_labels = len(labels)\n",
    "list_mean_inside = []\n",
    "list_mean_outside = []\n",
    "for el in tqdm(candidates[\"list_users\"], total=len(candidates[\"list_users\"]), desc=\"Computing in/out probs\"):\n",
    "    # print(f\"Selected IDs: {sel_ids}\")\n",
    "    mean_inside, mean_outside = sequential_compute_in_out_probs(labels, el, tot_num_labels, sum_all_labels)\n",
    "    list_mean_inside.append(mean_inside)\n",
    "    list_mean_outside.append(mean_outside)\n",
    "    # print(f\"Positive rate inside: {mean_inside}, Positive rate outside: {mean_outside}\")\n",
    "\n",
    "candidates[\"in_rate_debug\"], candidates[\"out_rate_debug\"] = list_mean_inside, list_mean_outside"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
