{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbcaf7c-64f4-41fe-b59c-647fa3fddd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12c9d1d-dbd8-46bd-a408-b343e6927819",
   "metadata": {},
   "source": [
    "### Main code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b905fba",
   "metadata": {},
   "source": [
    "Read the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd059a8-2d5d-4300-9a63-ab29d6dae830",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the files needed in input\n",
    "path_users_classification = './data_simulator/huge_dataset/dataset_simulator_trajectories.compressed.parquet.classified.parquet'\n",
    "path_users_cells_mapping = './mapping_users_cells_grid_50m.pkl'\n",
    "\n",
    "users_labels = pd.read_parquet(path_users_classification)\n",
    "display(users_labels)\n",
    "\n",
    "mapping_users_cells = pd.DataFrame(pd.read_pickle(path_users_cells_mapping))\n",
    "display(mapping_users_cells)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5ff090",
   "metadata": {},
   "source": [
    "### Various operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03b13c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming that we are dealing with the classification case, and that the labels follow a Bernoullian distribution, \n",
    "# determine the global positive and negative rates on the users labels.\n",
    "global_positive_rate = users_labels['label'].mean()\n",
    "global_negative_rate = 1 - global_positive_rate\n",
    "print(f\"Global positive rate: {global_positive_rate} - global negative rate: {global_negative_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493988e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the labels to the dataframe that maps 'users to cells'.\n",
    "mapping_users_cells['label'] = users_labels['label']\n",
    "display(mapping_users_cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a033c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce a more compact indexing for the cell IDs: these IDs can have gaps in them, so reindex their IDs\n",
    "# to prepare more efficient set intersections over the user IDs they refer to.\n",
    "array_cell_ids = np.sort(mapping_users_cells['cell_id'].unique())\n",
    "remapping_indices_cells = pd.Series(index = array_cell_ids, data = range(len(array_cell_ids)))\n",
    "del array_cell_ids\n",
    "display(remapping_indices_cells)\n",
    "\n",
    "# Produce a more compact indexing for the user IDs. Same reason as above.\n",
    "array_user_ids = np.sort(mapping_users_cells.index.unique())\n",
    "remapping_indices_users = pd.Series(index = array_user_ids, data = range(len(array_user_ids)))\n",
    "del array_user_ids\n",
    "display(remapping_indices_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c25d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "remapping_users_cells = mapping_users_cells.copy(deep=True)\n",
    "\n",
    "# Remap cell and user IDs to comntinous ranges.\n",
    "remapping_users_cells.index = remapping_users_cells.index.map(remapping_indices_users)\n",
    "display(remapping_users_cells)\n",
    "remapping_users_cells['cell_id'] = remapping_users_cells['cell_id'].map(remapping_indices_cells)\n",
    "display(remapping_users_cells)\n",
    "\n",
    "# Regenerate the mapping between users-labels.\n",
    "remapping_users_labels = remapping_users_cells.groupby('uid')['label'].first()\n",
    "display(remapping_users_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8cbb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute some aggregations at cell-level, effectively creating an augmented version of the grid. \n",
    "stats_config = {'list_users' : pd.NamedAgg(column='uid', aggfunc=set),\n",
    "                'num_users' : pd.NamedAgg(column='uid', aggfunc='nunique'),\n",
    "                'positive_rate' : pd.NamedAgg(column='label', aggfunc='mean')}\n",
    "aug_grid = (remapping_users_cells.reset_index()\n",
    "                                 .groupby('cell_id')\n",
    "                                 .agg(**stats_config))\n",
    "\n",
    "\n",
    "# Sort the cells by their IDs.\n",
    "aug_grid.sort_values(by='cell_id', ascending=True, inplace=True)\n",
    "display(aug_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b46544c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10de4001",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_combinations_cells_tocheck = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95da6fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Versione Python non ottimizzata intersezione liste ID utenti coppia celle.\n",
    "# TODO: questa parte puo' essere quasi interamente implementata con un algoritmo per il frequent itemset mining, e.g., apriori.\n",
    "#       Ad esempio, una possibile implementazione in Python e' https://github.com/tommyod/Efficient-Apriori/tree/master.\n",
    "from itertools import combinations\n",
    "\n",
    "intersections = {}\n",
    "cnt_threshold = 5 # TODO: da settare in funzione dello statistical power che vogliamo nel test d'ipotesi.\n",
    "pairs = zip(aug_grid.index, aug_grid['list_users'])\n",
    "for (cell_id, list_users), (other_cell_id, other_list_users) in combinations(pairs, 2):\n",
    "    \n",
    "    # Compute the set intersection, and its cardinality.\n",
    "    intersection = list_users & other_list_users\n",
    "    cnt = len(intersection)\n",
    "\n",
    "    # Add to the dictionary only the cell pairs that have at least 'threshold' users in common.\n",
    "    # The threshold should be calculated according to the statistical power we want to have in the hypotesis tests.\n",
    "    if cnt > cnt_threshold : intersections[(cell_id, other_cell_id)] = intersection\n",
    "\n",
    "\n",
    "# Store the results of the set intersections in a pandas Dataframe.\n",
    "res_intersections = pd.Series(data = intersections, name='list_users').to_frame()\n",
    "del intersections\n",
    "\n",
    "display(res_intersections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eee0202",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have to compute the local positive rate of the various combinations of cells...\n",
    "\n",
    "# 1 - turn each user ID in a list element into a row.\n",
    "tmp = res_intersections.explode('list_users', ignore_index=False)\n",
    "display(tmp)\n",
    "\n",
    "# 2 - For every user ID, find the associated predicted label.\n",
    "tmp['labels'] = tmp['list_users'].map(remapping_users_labels)\n",
    "display(tmp)\n",
    "\n",
    "# 3- For every combination of cells found in tmp's index, compute the local positive rate.\n",
    "res_intersections['positive_rate'] = tmp.groupby(level=list(range(tmp.index.nlevels)))['labels'].mean()\n",
    "del tmp\n",
    "display(res_intersections)\n",
    "\n",
    "\n",
    "# TODO: add to the list of combinations  of cell to check those whose positive rate differs more than\n",
    "#       some threshold from the global one.\n",
    "eps = 0.2\n",
    "combs_cells_tocheck = res_intersections[abs(res_intersections['positive_rate'] - global_positive_rate) > eps]\n",
    "print(f\"Number of combinations of cells to test: {len(combs_cells_tocheck)}\")\n",
    "display(combs_cells_tocheck)\n",
    "\n",
    "list_combinations_cells_tocheck.extend(combs_cells_tocheck.index.to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ffcd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Codice generico per intersezione liste ID utenti di combinazione celle con lista utenti cella singola.\n",
    "# TODO: da mettere a posto anche per il caso in cui dim_itemset=1 (caso base) e poi incapsulare in una classe.\n",
    "dim_itemset = res_intersections.index.nlevels\n",
    "\n",
    "base_lvls = list(range(dim_itemset - 1))\n",
    "intersections = {}\n",
    "for keys, sub in res_intersections.loc[:, 'list_users'].groupby(level=base_lvls, sort=False) :\n",
    "    # If the multi-index has size 2, then the first \"n-1\" levels is just one level, and thus isn't a tuple.\n",
    "    if dim_itemset - 1 == 1 : keys = (keys,)\n",
    "\n",
    "    # Drop the first 'n-1' levels in the multi-index, leaving only the last one.\n",
    "    s = sub.droplevel(base_lvls)\n",
    "    # display(s)\n",
    "\n",
    "    # Generate all the possible tuples of length 'n+1' by keeping fixed the first \"n-1\" keys and \n",
    "    # consider all the possible combinations of length 2 that can be generated in the last level of the\n",
    "    # multi-index.\n",
    "    for a, b in combinations(s.index, 2):\n",
    "        # Compute the set intersection, and its cardinality.\n",
    "        intersection = s[a] & s[b]\n",
    "        cnt = len(intersection)\n",
    "\n",
    "        # Add to the dictionary only the cell pairs that have at least 'threshold' users in common.\n",
    "        # The threshold should be calculated according to the statistical power we want to have in the hypotesis tests.\n",
    "        if cnt > cnt_threshold : intersections[(*keys, a, b)] = intersection\n",
    "\n",
    "\n",
    "new_res_intersections = pd.Series(data = intersections, name='list_users').to_frame()\n",
    "del intersections\n",
    "display(new_res_intersections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6576089d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: quindi, usare il codice gia' presente in una delle celle sopra per aggiungere ad una lista quelle combinazioni di celle\n",
    "#       per cui giudicheremo necessario effettuare il test statistico."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
