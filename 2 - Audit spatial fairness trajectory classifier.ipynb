{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing a Trajectory Classifier for Spatial Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import folium\n",
    "\n",
    "import sys\n",
    "sys.path.append('./src')    \n",
    "from functions import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the output of a trajectory classifier\n",
    "\n",
    "Here we load a dataset that should contain the labels given by a classifier to trajectories. In the dataset, we expect to find:\n",
    "\n",
    "1. the labels given to each trajectory;\n",
    "2. the sequence of points associated with each trajectory.\n",
    "\n",
    "**NOTE**: for now we use the LAR (`data/LAR.csv`) for development purposes, which refers to points and not trajectories. It contains the modified Loan/Application Register records in the US for Bank of America for the year 2021; the dataset is created by `data/LAR/create_LAR.ipynb`.\n",
    "\n",
    "**TODO**: adapt the code to a dataset of trajectories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>action_taken</th>\n",
       "      <th>census_tract</th>\n",
       "      <th>location</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>10003015200</td>\n",
       "      <td>(39.7070504, -75.5832416)</td>\n",
       "      <td>39.707050</td>\n",
       "      <td>-75.583242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>6059086502</td>\n",
       "      <td>(33.8503559, -117.9121351)</td>\n",
       "      <td>33.850356</td>\n",
       "      <td>-117.912135</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>26163596100</td>\n",
       "      <td>(42.1014482, -83.1602786)</td>\n",
       "      <td>42.101448</td>\n",
       "      <td>-83.160279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>9009165600</td>\n",
       "      <td>(41.3558996, -72.9323558)</td>\n",
       "      <td>41.355900</td>\n",
       "      <td>-72.932356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>36061001200</td>\n",
       "      <td>(40.7159065, -73.9820936)</td>\n",
       "      <td>40.715907</td>\n",
       "      <td>-73.982094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   action_taken  census_tract                    location        lat  \\\n",
       "0             1   10003015200   (39.7070504, -75.5832416)  39.707050   \n",
       "1             1    6059086502  (33.8503559, -117.9121351)  33.850356   \n",
       "2             1   26163596100   (42.1014482, -83.1602786)  42.101448   \n",
       "3             1    9009165600   (41.3558996, -72.9323558)  41.355900   \n",
       "4             3   36061001200   (40.7159065, -73.9820936)  40.715907   \n",
       "\n",
       "          lon  \n",
       "0  -75.583242  \n",
       "1 -117.912135  \n",
       "2  -83.160279  \n",
       "3  -72.932356  \n",
       "4  -73.982094  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=206418 points\n",
      "P=127286 positives\n"
     ]
    }
   ],
   "source": [
    "## load the dataset\n",
    "df = load_data('./data/LAR.csv')\n",
    "display(df.head())\n",
    "\n",
    "# Set the name of the column containing the labels given to the trajectories.\n",
    "label = 'action_taken'\n",
    "N, P = get_stats(df, label)\n",
    "\n",
    "# Print some general statistics about the points and labels. Here, the positive label is \"1\", the other ones are considered negative.\n",
    "print(f'N={N} points')\n",
    "print(f'P={P} positives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rtree = create_rtree(df)\n",
    "# rtree = create_rtree_v2(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lat_max = df['lat'].values.max()\n",
    "# lat_min = df['lat'].values.min()\n",
    "# lon_max = df['lon'].values.max()\n",
    "# lon_min = df['lon'].values.min()\n",
    "\n",
    "# mapit = folium.Map(location=[37.09, -95.71], zoom_start=5, tiles=\"Stamen Toner\")\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     if row[label] == 1:\n",
    "#         folium.CircleMarker( location=(row['lat'], row['lon']), color='#00FF00', fill_color='#00FF00', fill=True, opacity=0.4, fill_opacity=0.4, radius=2 ).add_to( mapit )\n",
    "#     elif row[label] == 0:\n",
    "#         folium.CircleMarker( location=(row['lat'], row['lon']), color='#FF0000', fill_color='#FF0000', fill=True, opacity=0.4, fill_opacity=0.4, radius=2 ).add_to( mapit )\n",
    "\n",
    "\n",
    "# mapit.fit_bounds([(lat_min, lon_min), (lat_max, lon_max)])\n",
    "\n",
    "# mapit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Run Experiments\n",
    "\n",
    "There are three experiments:\n",
    "- Unrestricted regions: runs **our approach** on unrestricted regions.\n",
    "- One Partitioning: runs **our approach** against **MeanVar** on regions from a single partitioning.\n",
    "- Multiple Partitionings: runs **MeanVar** on multiple partitionings.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unrestricted regions (corresponds to Sec. 4.3 of the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a set of points (locations), cluster them and then consider the clusters' centroids. Then, for each\n",
    "# centroid find the nearest location in the r-tree: this point will be used as a seed .\n",
    "seeds = create_seeds(df, rtree, 100)\n",
    "print(len(seeds), seeds[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given a set of seed point IDs and a list of radii, this function creates candidate regions. \n",
    "# For each seed and radius, it queries the spatial index (using query_range) to get all points within a square centered around\n",
    "# the seed and with side 2*radius. This square corresponds to a region, and packages the information into a dictionary.\n",
    "# Purpose: To generate many regions whose fairness (or lack thereof) can be audited.\n",
    "\n",
    "radii = np.arange(0.05, 1.01, 0.05)\n",
    "regions = create_regions(df, rtree, seeds, radii)\n",
    "\n",
    "print(len(regions), 'regions')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mapit = folium.Map(location=[37.09, -95.71], zoom_start=5, prefer_canvas = True, tiles='cartodbpositron')\n",
    "\n",
    "# Plot the seeds on a map.\n",
    "for point in seeds:\n",
    "    # NOTE: id2loc retrieve from 'df' a specific location according to its ID in 'point'.\n",
    "    folium.CircleMarker(location=id2loc(df, point), color='#0000FF', fill_color='#0000FF', fill=True, opacity=0.4, fill_opacity=0.4, radius=2 ).add_to( mapit )\n",
    "\n",
    "\n",
    "# NOTE: The code below plots two rectangles in the ocean, and has been probably used just for debugging purposes.\n",
    "\n",
    "#center = np.array([26, -126])\n",
    "#r = radii[0]\n",
    "#lower_left = center - r\n",
    "#upper_right = center + r\n",
    "#folium.Rectangle([lower_left, upper_right], color='#F1CF3B').add_to( mapit )\n",
    "\n",
    "#r = radii[-1]\n",
    "#lower_left = center - r\n",
    "#upper_right = center + r\n",
    "#folium.Rectangle([lower_left, upper_right], color='#F1CF3B').add_to( mapit )\n",
    "\n",
    "\n",
    "mapit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "direction = 'both'\n",
    "# direction = 'less_in'\n",
    "# direction = 'less_out'\n",
    "\n",
    "best_region, max_likeli, statistics = scan_regions(regions, true_types, N, P, direction=direction, verbose=True)\n",
    "\n",
    "# statistics.sort(key=lambda x: -x)\n",
    "# print(statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## determine the significance threshold based on a desired signif_level\n",
    "n_alt_worlds = 200\n",
    "signif_level = 0.005\n",
    "\n",
    "signif_thresh = get_signif_threshold(signif_level, n_alt_worlds, regions, N, P)\n",
    "print(signif_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## identify regions with test statistic above statistical significance threshold\n",
    "\n",
    "# Sort the max likelihood ratios computed when considering the real data.\n",
    "sorted_statistics = np.sort(statistics)\n",
    "\n",
    "# Now use the threshold computed from the simulations, to find out the number of regions, 'top_k',\n",
    "# for which the null hypotesis is false (according to the given \\alpha = signif_thresh)\n",
    "# when considering max likelihood ration computed from the real data. In other words,\n",
    "# we are determining the number of regions for which the considered classifier *is not spatially fair*.\n",
    "top_k = len(statistics) - np.searchsorted(sorted_statistics, signif_thresh)\n",
    "print(top_k, 'significant regions')\n",
    "\n",
    "# Compute the list of indices that sort the array 'statistics'.\n",
    "# In essence, 'np.argsort' perform an indirect sort along the given axis; it returns an array of indices of the same shape\n",
    "# as the array being sorted that index data along the given axis in sorted order.\n",
    "indexes = np.argsort(statistics)[::-1][:top_k]\n",
    "\n",
    "# Select the 'top_k' regions that are not spatially fair\n",
    "significant_regions = [ regions[i] for i in indexes ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersects(regionA, regionB):\n",
    "    cA = np.array(id2loc(df, regionA['center']))\n",
    "    cB = np.array(id2loc(df, regionB['center']))\n",
    "    rA = regionA['radius']\n",
    "    rB = regionB['radius']\n",
    "\n",
    "    A_top_right = cA + np.array([rA, rA])\n",
    "    A_bottom_left = cA - np.array([rA, rA])\n",
    "    B_top_right = cB + np.array([rB, rB])\n",
    "    B_bottom_left = cB - np.array([rB, rB])\n",
    "\n",
    "    # print(A_bottom_left, A_top_right, B_bottom_left, B_top_right)\n",
    "\n",
    "    return not (A_top_right[0] < B_bottom_left[0] or A_bottom_left[0] > B_top_right[0] or A_top_right[1] < B_bottom_left[1] or A_bottom_left[1] > B_top_right[1])\n",
    "\n",
    "\n",
    "\n",
    "non_olap_regions = []\n",
    "centers = []\n",
    "for region in significant_regions:\n",
    "    center = region['center']\n",
    "    if center in centers:\n",
    "        continue\n",
    "    \n",
    "    no_intersections = True\n",
    "    for other in non_olap_regions:\n",
    "        if intersects(region, other):\n",
    "            no_intersections = False\n",
    "            break\n",
    "    if no_intersections:\n",
    "        centers.append(center)\n",
    "        non_olap_regions.append(region)\n",
    "    # print(region['radius'])\n",
    "\n",
    "print(len(non_olap_regions), 'non-overlapping regions')\n",
    "\n",
    "# over(non_olap_regions[0], non_olap_regions[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## find smallest, largest regions\n",
    "\n",
    "min_radius = np.inf\n",
    "max_radius = -np.inf\n",
    "for region in non_olap_regions:\n",
    "    if region['radius'] < min_radius:\n",
    "        min_radius = region['radius']\n",
    "        # region_min_radius = region\n",
    "    if region['radius'] > max_radius:\n",
    "        max_radius = region['radius']\n",
    "        # region_max_radius = region\n",
    "\n",
    "min_points = np.inf\n",
    "max_points = -np.inf\n",
    "for region in non_olap_regions:\n",
    "    if region['radius'] == min_radius and len(region['points']) < min_points:\n",
    "        min_points = len(region['points'])\n",
    "        region_min_radius = region\n",
    "    if region['radius'] == max_radius and len(region['points']) > max_points:\n",
    "        max_points = len(region['points'])\n",
    "        region_max_radius = region\n",
    "\n",
    "print(len(region_min_radius['points']), len(region_max_radius['points']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_circular_regions(df, true_types, non_olap_regions[:5])\n",
    "\n",
    "# show_circular_regions(df, true_types, [region_min_radius, region_max_radius])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Partitioning evaluation\n",
    "\n",
    "Instead of creating regions from clusters of trajectories/points, here we use a more common approach.\n",
    "We superimpose a uniform grid over the space: the grid's cells will be the regions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aux functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_partitioning(df, rtree, lon_min: float, lon_max: float, lat_min: float, lat_max: float, lon_n: float, lat_n: float):\n",
    "    grid_info = {}\n",
    "    grid_info['lon_min'] = lon_min\n",
    "    grid_info['lon_max'] = lon_max\n",
    "    grid_info['lat_min'] = lat_min\n",
    "    grid_info['lat_max'] = lat_max\n",
    "    grid_info['lat_n'] = lat_n\n",
    "    grid_info['lon_n'] = lon_n\n",
    "\n",
    "    grid_loc2_idx = {} ## maps (x,y) grid_loc coords to an index in the partitions array\n",
    "\n",
    "    partitions = []\n",
    "    for i in range(lat_n):\n",
    "        lat_start = lat_min + (i/lat_n)*(lat_max - lat_min)\n",
    "        lat_end = lat_min + ((i+1)/lat_n)*(lat_max - lat_min)\n",
    "        for j in range(lon_n):\n",
    "            lon_start = lon_min + (j/lon_n)*(lon_max - lon_min)\n",
    "            lon_end = lon_min + ((j+1)/lon_n)*(lon_max - lon_min)\n",
    "\n",
    "            points = query_range_box(df, rtree, lon_start, lon_end, lat_start, lat_end)\n",
    "            # print(len(points))\n",
    "            partition  = {\n",
    "                'grid_loc': (j, i),\n",
    "                'points' : points,\n",
    "            }\n",
    "            grid_loc2_idx[(j,i)] = len(partitions)\n",
    "            partitions.append(partition)\n",
    "    \n",
    "    return grid_info, grid_loc2_idx, partitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the bounding box in which the points/trajectories are.\n",
    "lat_max = df['lat'].values.max()\n",
    "lat_min = df['lat'].values.min()\n",
    "lon_max = df['lon'].values.max()\n",
    "lon_min = df['lon'].values.min()\n",
    "print(lat_min, lat_max, lon_min, lon_max)\n",
    "\n",
    "\n",
    "### create the partitioning (grid) and its partitions (regions)\n",
    "\n",
    "lat_n = 20 ## number of partitions along vertical axis (latitude)  ## was 12\n",
    "lon_n = 20 ## number of partitions along horizontal axis (longitude) ## was 25\n",
    "\n",
    "grid_info, grid_loc2_idx, regions = create_partitioning(df, rtree, lon_min, lon_max, lat_min, lat_max, lon_n, lat_n)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audit the classifier's output with our approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_region, max_likeli, statistics = scan_regions(regions, true_types, N, P, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## determine the significance threshold based on a desired signif_level\n",
    "n_alt_worlds = 200\n",
    "signif_level = 0.005\n",
    "\n",
    "signif_thresh = get_signif_threshold(signif_level, n_alt_worlds, regions, N, P)\n",
    "print(signif_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## identify regions with statistic above statistical significance threshold\n",
    "\n",
    "sorted_statistics = np.sort(statistics)\n",
    "# print(sorted_statistics[::-1][40:60])\n",
    "# print(np.sort(statistics)[::-1][40:60])\n",
    "\n",
    "top_k = len(statistics) - np.searchsorted(sorted_statistics, signif_thresh)\n",
    "\n",
    "print(top_k, 'significant regions')\n",
    "\n",
    "\n",
    "indexes = np.argsort(statistics)[::-1][:top_k]\n",
    "\n",
    "significant_regions = [ regions[i] for i in indexes ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_grid_region(df, grid_info, true_types, best_region)\n",
    "show_grid_regions(df, grid_info, true_types, significant_regions[:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## best_region vs the_region\n",
    "\n",
    "the_region = top_regions[0]\n",
    "\n",
    "best_idx = grid_loc2_idx[best_region['grid_loc']]\n",
    "the_idx = grid_loc2_idx[the_region['grid_loc']]\n",
    "\n",
    "print(best_region['grid_loc'], the_region['grid_loc'])\n",
    "print(best_idx, the_idx)\n",
    "\n",
    "print(statistics[best_idx], statistics[the_idx])\n",
    "print(scores[best_idx], scores[the_idx])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try out different multiple Partitionings\n",
    "\n",
    "Teoricamente non serve con metodi basati su test d'ipotesi (era usato con il solo MeanVar), ma eventualmente si puo' reintegrare dal notebook originale."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
