{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing a Trajectory Classifier for Spatial Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import folium\n",
    "\n",
    "from src.functions import *\n",
    "from src.traj_functions import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the output of a trajectory classifier\n",
    "\n",
    "Here we load a dataset that should contain the labels given by a classifier to trajectories. In the dataset, we expect to find:\n",
    "\n",
    "1. the labels given to each trajectory;\n",
    "2. the sequence of points associated with each trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>geometry</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LINESTRING (2.52375 48.96159, 2.52416 48.9611,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LINESTRING (2.54396 48.96144, 2.54069 48.9604,...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LINESTRING (2.33953 48.87304, 2.33957 48.87295...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LINESTRING (2.32876 48.87064, 2.32874 48.87055...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LINESTRING (2.33999 48.86894, 2.34002 48.86897...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            geometry  label\n",
       "0  LINESTRING (2.52375 48.96159, 2.52416 48.9611,...      0\n",
       "1  LINESTRING (2.54396 48.96144, 2.54069 48.9604,...      1\n",
       "2  LINESTRING (2.33953 48.87304, 2.33957 48.87295...      1\n",
       "3  LINESTRING (2.32876 48.87064, 2.32874 48.87055...      0\n",
       "4  LINESTRING (2.33999 48.86894, 2.34002 48.86897...      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N=2738 points, P=1635 positives, frac.positives=0.60\n"
     ]
    }
   ],
   "source": [
    "## Load the labelled trajectory dataset.\n",
    "dataset_path = './synth_traj_dataset_fair.pkl'\n",
    "gdf_trajs = gpd.GeoDataFrame(pd.read_pickle(dataset_path))\n",
    "display(gdf_trajs.head())\n",
    "\n",
    "# Set the name of the column containing the labels given to the trajectories.\n",
    "label = 'label'\n",
    "N, P = get_stats(gdf_trajs, label)\n",
    "\n",
    "# Function below remaps labels different than \"1\" to \"0\". Used to reduce multi-class classification to binary classification.\n",
    "true_types = get_true_types(gdf_trajs, label)\n",
    "# print(true_types[:30])\n",
    "\n",
    "# Print some general statistics about the points and labels. Here, the positive label is \"1\", the other ones are considered negative.\n",
    "print(f'N={N} points, P={P} positives, frac.positives={P/N:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lat_max = df['lat'].values.max()\n",
    "# lat_min = df['lat'].values.min()\n",
    "# lon_max = df['lon'].values.max()\n",
    "# lon_min = df['lon'].values.min()\n",
    "\n",
    "# mapit = folium.Map(location=[37.09, -95.71], zoom_start=5, tiles=\"Stamen Toner\")\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     if row[label] == 1:\n",
    "#         folium.CircleMarker( location=(row['lat'], row['lon']), color='#00FF00', fill_color='#00FF00', fill=True, opacity=0.4, fill_opacity=0.4, radius=2 ).add_to( mapit )\n",
    "#     elif row[label] == 0:\n",
    "#         folium.CircleMarker( location=(row['lat'], row['lon']), color='#FF0000', fill_color='#FF0000', fill=True, opacity=0.4, fill_opacity=0.4, radius=2 ).add_to( mapit )\n",
    "\n",
    "\n",
    "# mapit.fit_bounds([(lat_min, lon_min), (lat_max, lon_max)])\n",
    "\n",
    "# mapit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Run Experiments\n",
    "\n",
    "There are three experiments:\n",
    "- (TODO) Unrestricted regions: runs **our approach** on unrestricted regions.\n",
    "- Regions are cells of a uniform grid: evaluates **our approach** over a single partitioning.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Partitioning evaluation\n",
    "\n",
    "Instead of creating regions from clusters of trajectories/points, here we use a more common approach.\n",
    "We superimpose a uniform grid over the space: the grid's cells will be the regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0067567, 48.71535, 2.599733, 48.9782447\n"
     ]
    }
   ],
   "source": [
    "### create the partitioning (grid) and its partitions (regions)\n",
    "num_slices_lon = 20\n",
    "num_slices_lat = 20\n",
    "grid_info, grid_loc2_idx, regions = create_traj_partitioning(gdf_trajs, num_slices_lon, num_slices_lat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audit the classifier's output with our approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "range 0.0 5.145969938050712\n",
      "max likelihood 5.145969938050712\n"
     ]
    }
   ],
   "source": [
    "best_region, max_likeli, statistics = scan_regions(regions, true_types, N, P, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.762502034416457\n"
     ]
    }
   ],
   "source": [
    "# Determine the significance threshold based on a desired signif_level\n",
    "# Example: if we conduct 200 simulations, and we have a signif_level of 0.005, then we are interested in\n",
    "#          regions whose test statistics is larger than the top 200*0.005 = 1st test statistics obtained\n",
    "#          in the simulations.\n",
    "n_alt_worlds = 200\n",
    "signif_level = 0.005\n",
    "signif_thresh = get_signif_threshold(signif_level, n_alt_worlds, regions, N, P)\n",
    "print(signif_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of regions: 400) <class 'numpy.float64'>\n",
      "0 significant regions\n",
      "Argsort: [ 16  17  18 380  44 360  77  48 365 339 340 284 181 180 160 161 384  15\n",
      " 392 391 394 393 242 241 200  12 139  65  57 110 359  42  80 326 100 105\n",
      " 214 395 396 288 121 213 290 272 195   8 353  46 261 303 146 197 135 296\n",
      "  28 159 358 319 109  50 271  14  36 304 108 348 285 370 141 256  27 334\n",
      " 163 268 210 273 267 291 130 311 344  63  43  64 259 218 102 266 283 318\n",
      " 276  37 123 250  13 137  71   1  40 390   0 382 317  26 245 134 229 253\n",
      " 192 294 125 157 111 270 131 274  31   9 282 120 179 293 251 196 362 118\n",
      "  21  20 237 224 238 113 147 235  67 103 107 183 158 305 337 151  85 328\n",
      "  51 133 209  25 138  95 286 112 309 292 275 314  30 366  29  90 115 338\n",
      " 376  75 172 162 333 247 132 122  47  45 280  19 219  35 297 281  41  89\n",
      " 116  55  91 227  33 299 211 265 225 142 374 182 212 330  53 144  54 236\n",
      " 216 308   7  86 232  10 316 287  49 104 260 355  87 336  56 243 295  76\n",
      " 298  11 397 301  24 168 343 323 321 341 258 223 264 166 302 346 106   5\n",
      " 354  96 233 202  73 347   2 375 207 153 289 379  59 356  39 140 320  79\n",
      " 244 383 129 198 184 313 173 248  32 215 145 230 206 226 189 257 231 101\n",
      " 249  34 136 252 269 190 154 199 329 398 239 312 117 262 331 335 167 277\n",
      " 307 234 342 300  22   4 315 171 193 222 361 381 324 389 332 170 255 386\n",
      " 150  74 254 203 322  97  70 278 364  62 345 357  78 143  58 373  38 363\n",
      " 174 187 156 208 221 201  60 399 372  61 240 220  88 178 152 114 377  23\n",
      "  66 350 327 263 351 246  69 191 186 194 165 124 155 175  84 119 325  99\n",
      " 367 279 164 228 310 176 378 349  94 177 385 368 371 217 205 126 387  68\n",
      " 388  98   3  81 188 185 369 306 352  52 149 127  82  83 128   6 169 204\n",
      "  92 148  72  93]\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of regions: {len(statistics)}) {type(statistics[0])}')\n",
    "\n",
    "# Identify regions with test statistic above the statistical significance threshold\n",
    "sorted_statistics = np.sort(statistics)\n",
    "top_k = len(statistics) - np.searchsorted(sorted_statistics, signif_thresh)\n",
    "print(top_k, 'significant regions')\n",
    "\n",
    "# Retrieve the indexes of the anomalous regions. Then, retrieve their details.\n",
    "indexes = np.argsort(statistics)[::-1][:top_k]\n",
    "significant_regions = [regions[i] for i in indexes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_grid_region(df, grid_info, true_types, best_region)\n",
    "show_grid_regions(gdf_trajs, grid_info, true_types, significant_regions[:])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## best_region vs the_region\n",
    "\n",
    "the_region = top_regions[0]\n",
    "\n",
    "best_idx = grid_loc2_idx[best_region['grid_loc']]\n",
    "the_idx = grid_loc2_idx[the_region['grid_loc']]\n",
    "\n",
    "print(best_region['grid_loc'], the_region['grid_loc'])\n",
    "print(best_idx, the_idx)\n",
    "\n",
    "print(statistics[best_idx], statistics[the_idx])\n",
    "print(scores[best_idx], scores[the_idx])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
