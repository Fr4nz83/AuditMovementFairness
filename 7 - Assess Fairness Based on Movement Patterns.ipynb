{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing a Classifier for Fairness Based on Movement Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aux functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main code\n",
    "\n",
    "Read the dataset with the objects' labels.\n",
    "**TODO**: we are using dummy labels for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_candidates = './data_simulator/huge_dataset/gencand/'\n",
    "\n",
    "with open(path_candidates + \"dict_flattened_candidates.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "### Load the dictory containing the flattened objects ID lists associated with the candidates ###\n",
    "flat_ids, indptr, lens = data['flat_ids'], data['start_pos'], data['lengths']\n",
    "del data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build a single numpy vector contaning the candidates of all the grids. Then, flatten the lists in the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset containing the true labels of the objects.\n",
    "n_objects = 100000\n",
    "positive_rate = 0.6\n",
    "labels = np.random.binomial(n=1, p=positive_rate, size=n_objects).astype(np.int8)\n",
    "# labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import xlogy, xlog1py\n",
    "\n",
    "def batch_max_likelihood_ratio(labels_objects: np.ndarray, \n",
    "                               flat_ids: np.ndarray, indptr: np.ndarray, lens: np.ndarray,\n",
    "                               tot_sum_labels: int,\n",
    "                               logL0_max: float) -> tuple[np.ndarray, np.ndarray, np.ndarray, float]:\n",
    "    \n",
    "\n",
    "    # Gather labels for all ids, then sum per candidate via segmented reduction.\n",
    "    flat_vals = labels_objects[flat_ids]\n",
    "    inside_sum = np.add.reduceat(flat_vals, indptr[:-1]).astype(np.float32, copy=False)\n",
    "\n",
    "\n",
    "    # Vectorized computation: for each candidate subset of cells, compute the positive rate of the objects\n",
    "    # associated with the subset vs the positive rate of the other objects.\n",
    "    # NOTE: we use np.divide with the `where` parameter to avoid divisions by zero.\n",
    "    p, n = inside_sum, lens\n",
    "    P, N = tot_sum_labels, labels_objects.size\n",
    "    inside_positive_rate  = np.divide(p, n, out=np.zeros_like(p, dtype=np.float32), where=(n > 0))\n",
    "    outside_positive_rate = np.divide(P - p, N - n, out=np.zeros_like(p, dtype=np.float32), where=((N - n) > 0))\n",
    "    \n",
    "\n",
    "    # Unsafe computation of the log-likelihood under the alternative hypotesis.\n",
    "    #logL1 = (p * np.log(inside_positive_rate)\n",
    "    #         + (n - p) * np.log1p(-inside_positive_rate)\n",
    "    #         + (P - p) * np.log(outside_positive_rate)\n",
    "    #         + (N - n - (P - p)) * np.log1p(-outside_positive_rate))\n",
    "    \n",
    "    \n",
    "    # Safe alternative computation of the log-L1 via scipy functions. \n",
    "    # NOTE: the log-likelihood is -inf when the positive rate is 0 or 1, which can happen when p==0 or p==n for the inside positive rate, \n",
    "    # or when P-p==0 or N-n-(P-p)==0 for the outside positive rate. This is not a problem per se, since we are interested in the likelihood\n",
    "    # ratio, and if the likelihood under the alternative hypotesis is -inf, then the likelihood ratio will be 0, which is what we expect in\n",
    "    # these cases.\n",
    "    \n",
    "    # valid = (n > 0) & (n < N) # optional: mask degenerate windows (n==0 or n==N)\n",
    "    # logL1 = np.full_like(inside_positive_rate, -np.inf, dtype=np.float32)\n",
    "    logL1 = ( xlogy(p, inside_positive_rate) +                          # p * np.log(inside_positive_rate)\n",
    "              xlog1py((n - p), -inside_positive_rate) +                 # + (n - p) * np.log1p(-inside_positive_rate)\n",
    "              xlogy((P - p), outside_positive_rate) +                   # + (P - p) * np.log(outside_positive_rate)\n",
    "              xlog1py((N - n - (P - p)), -outside_positive_rate) )      # + (N - n - (P - p)) * np.log1p(-outside_positive_rate))\n",
    "\n",
    "    # Vectorized computation of the log-likelihood ratio of the candidates\n",
    "    logLR = logL1 - logL0_max\n",
    "    maxLogLR = float(np.nanmax(logLR)) \n",
    "\n",
    "    return inside_positive_rate, outside_positive_rate, logLR, maxLogLR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the empirical distribution of the considered test statistic with a certain number of Monte Carlo simulations.\n",
    "\n",
    "Here we perform the Monte Carlo simulations needed to determine the distribution of the test statistics under the assumption that the null hypothesis is true. The test statistics used is the maximum likelihood ratio computed across the regions of all the grids, while the likelihood function is the binomial-based one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = labels.sum(dtype=np.uint32) # Constant across permutations\n",
    "N = labels.size\n",
    "\n",
    "\n",
    "# Compute the L_0 likelihood, which models the likelihood of observing the labels in the data under the the assumption that the \n",
    "# null hypotesis H_0 is true, i.e., there is a single global distribution that governs the labels. L_0 is constant across \n",
    "# permutations, since it depends only on the total number of positive and negative labels in the dataset, which does not change\n",
    "# when shuffling the original labels.\n",
    "rho = P / N\n",
    "logL0_max = P * np.log(rho) + (N - P) * np.log1p(-rho)\n",
    "\n",
    "\n",
    "num_simulations = 200\n",
    "vec_max_LR = np.empty(num_simulations, dtype=np.float32)\n",
    "for i in tqdm(range(num_simulations)):    \n",
    "    # Shuffle the original labels assigned to the objects. This represents the null hypotesis H_0, according to which\n",
    "    # there is a single global distribution that governs the labels, i.e., there is not one or more sets of geographical regions\n",
    "    # in which the associated objects have an average positive rate that is significantly different than that of the other objects. \n",
    "    rng = np.random.default_rng(i)\n",
    "    shuffled_labels = rng.permutation(labels)\n",
    "\n",
    "    # For the objects associated with each subset of cells, compute their positive rate vs that of the other objects.\n",
    "    _, _, _, vec_max_LR[i] = batch_max_likelihood_ratio(shuffled_labels,\n",
    "                                                        flat_ids, indptr, lens,\n",
    "                                                        P, logL0_max)\n",
    "\n",
    "\n",
    "# DEBUG\n",
    "# np_list_candidates_inrate, np_list_candidates_outrate\n",
    "\n",
    "# Sort the max_LR distribution obtained via the simulations.\n",
    "# This is the distribution of the likelihood ratios of the most extreme regions observed empirically\n",
    "# under the assumption that the null hypothesis H_0 is true.\n",
    "sorted_vec_max_LR = np.sort(vec_max_LR)\n",
    "\n",
    "# Print the most extreme likelihood ratio observed across all the simulations.\n",
    "print(sorted_vec_max_LR[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now compute the max likelihood ratio from the candidates when considering the original labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inside_positive_rate, outside_positive_rate, vec_LR_dataset, max_LR_dataset = batch_max_likelihood_ratio(labels,\n",
    "                                                                                                         flat_ids, indptr, lens,\n",
    "                                                                                                         P, logL0_max)\n",
    "max_LR_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finally, determine if $H_0$ must be rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Significance level required (probability of rejecting the null when it is actually true).\n",
    "alpha = 0.05\n",
    "\n",
    "# Determine where the max LR computed with the original labels fall in the empirical test statistic's distribution.\n",
    "pos = np.searchsorted(sorted_vec_max_LR, max_LR_dataset, side=\"right\")  # where observed stat falls\n",
    "\n",
    "# Monte Carlo p-value of the observed test statistic's value derived from the ranked test statistic's distribution \n",
    "# (right tail), with +1 correction.\n",
    "p_value = (num_simulations - pos + 1) / (num_simulations + 1)\n",
    "\n",
    "# Based on the distribution and the real data we have, decide if we have to reject H_0.\n",
    "reject_H0 = p_value <= alpha\n",
    "\n",
    "print(f\"position in sorted MC sample: {pos}/{num_simulations}\")\n",
    "print(f\"Monte Carlo p-value: {p_value:.6f}\")\n",
    "print(\"Reject H0\" if reject_H0 else \"Do NOT reject H0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: plot the distribution of the likelihood ratios computed over the subsets of cells when considering the original labels.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(vec_LR_dataset, bins=200)  # tune bins (100â€“500 usually fine)\n",
    "plt.xlabel(\"Values of the candidates' likelihood ratios\")\n",
    "plt.ylabel(f\"Frequency (log-scale) (total: {vec_LR_dataset.size} objects)\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: find the characteristics of the subset of cells with the maximum LR when considering the original labels.\n",
    "\n",
    "# Find where the candidate with the max LR is located.\n",
    "idx = np.argsort(vec_LR_dataset)\n",
    "\n",
    "# Sort the 1D arrays of interest accordingly.\n",
    "vec_LR_dataset_sorted = vec_LR_dataset[idx]\n",
    "lens_sorted = lens[idx]\n",
    "inside_positive_rate_sorted = inside_positive_rate[idx]\n",
    "outside_positive_rate_sorted = outside_positive_rate[idx]\n",
    "\n",
    "# Print the max LR candidate's info.\n",
    "print(f\"Info most 'problematic' candidate: local ps: {inside_positive_rate_sorted[-1]}, \" +\n",
    "      f\"other ps: {outside_positive_rate_sorted[-1]}, num_objs_candidate: {lens_sorted[-1]}, \" +\n",
    "      f\"likelihood ratio: {vec_LR_dataset_sorted[-1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DEBUG: Simple for loop-based version of the computation of inside/outside positive rates; to be used for debugging purposes ###"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Simple for loop-based version ###\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "def sequential_compute_in_out_probs(labels: np.ndarray, sel_ids: np.ndarray, \n",
    "                                    tot_num_els : int, tot_sum_labels : float) -> tuple[float, float]:\n",
    "    \"\"\"\n",
    "    probs[i] = probability of object i\n",
    "    ids = iterable of object IDs (indices)\n",
    "\n",
    "    Returns:\n",
    "        (mean_inside, mean_outside)\n",
    "    \"\"\"\n",
    "\n",
    "    k = sel_ids.size\n",
    "    sum_inside = labels.take(sel_ids).sum(dtype=np.uint32)\n",
    "    mean_inside = float(sum_inside) / k\n",
    "    mean_outside = float(tot_sum_labels - sum_inside) / (tot_num_els - k)\n",
    "    \n",
    "    return mean_inside, mean_outside\n",
    "\n",
    "\n",
    "# Main code\n",
    "sum_all_labels = labels.sum()\n",
    "tot_num_labels = len(labels)\n",
    "list_mean_inside = []\n",
    "list_mean_outside = []\n",
    "for el in tqdm(candidates[\"list_users\"], total=len(candidates[\"list_users\"]), desc=\"Computing in/out probs\"):\n",
    "    # print(f\"Selected IDs: {sel_ids}\")\n",
    "    mean_inside, mean_outside = sequential_compute_in_out_probs(labels, el, tot_num_labels, sum_all_labels)\n",
    "    list_mean_inside.append(mean_inside)\n",
    "    list_mean_outside.append(mean_outside)\n",
    "    # print(f\"Positive rate inside: {mean_inside}, Positive rate outside: {mean_outside}\")\n",
    "\n",
    "candidates[\"in_rate_debug\"], candidates[\"out_rate_debug\"] = list_mean_inside, list_mean_outside"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
