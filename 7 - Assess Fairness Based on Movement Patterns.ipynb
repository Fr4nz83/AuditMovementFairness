{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing a Classifier for Fairness Based on Movement Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from src.bernoulli_spatial_scan import BernoulliSpatialScan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up an object modeling the Bernoulli-based spatial scan statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the flattened candidates + aux info.\n",
    "path_dict_candidates = './data_simulator/huge_dataset/gencand/dict_flattened_candidates.pkl' # Path to the flattened candidates.\n",
    "flat_ids, indptr, lengths = BernoulliSpatialScan.load_flattened_candidates(path_dict_candidates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the dataset with the \"true\" labels.\n",
    "**TODO**: we are using dummy labels for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset containing the true labels of the objects.\n",
    "n_objects = 100000\n",
    "positive_rate = 0.6\n",
    "labels = np.random.default_rng(42).binomial(n=1, p=positive_rate, size=n_objects).astype(np.int8)\n",
    "# labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First compute the Monte Carlo simulations needed to determine the distribution of the test statistics under the assumption that the null hypothesis is true. The test statistics used is the maximum log likelihood ratio computed across the regions of all the grids, while the likelihood functions to model H_0 and H_1 are the Bernoulli-based ones.\n",
    "\n",
    "Once we have the distribution, determine if the value of the test statistics computed from the \"real\" labels points to unfairness somewhere or not, thus rejecting the null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the 'BernoulliSpatialScan' object.\n",
    "num_simulations = 500   # Number of Monte Carlo simulations to derive an approx. distribution of the test statistics\n",
    "alpha = 0.02            # Significance level required.\n",
    "spatial_scan = BernoulliSpatialScan(num_simulations, alpha, flat_ids, indptr, lengths)\n",
    "#\n",
    "# reject, vec_max_LR_sims, dist_LR_labels, max_LR_labels = spatial_scan.sequential_simulations(labels)\n",
    "reject, vec_max_LR_sims, dist_LR_labels, max_LR_labels = spatial_scan.parallel_simulations(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: print some info about the results.\n",
    "print(reject, np.flip(np.sort(vec_max_LR_sims)[-int(vec_max_LR_sims.size * alpha) : ]), max_LR_labels)\n",
    "\n",
    "# Find out the indexes of the subsets of cells whose likelihood ratios with the original labels are above\n",
    "# the simulations' threshold for which they should be considered 'extreme' and the null hypothesis must be rejected.\n",
    "threshold_value = np.sort(vec_max_LR_sims)[-int(vec_max_LR_sims.size * alpha)]\n",
    "print(threshold_value)\n",
    "pos_extreme_candidates = np.argwhere(dist_LR_labels >= threshold_value).tolist()\n",
    "print(pos_extreme_candidates)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
