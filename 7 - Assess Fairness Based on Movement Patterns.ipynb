{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing a Classifier for Fairness Based on Movement Patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "from scipy.special import xlogy, xlog1py\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bernoulli-based spatial scan statistic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_simulations = 500 # Number of Monte Carlo simulations to derive an approx. distribution of the test statistics\n",
    "alpha = 0.05          # Significance level required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main code\n",
    "\n",
    "Read a dictionary containing the candidates (subsets of cells with at least an associated object) of all the grids; the candidates are in a 1D flattened array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_candidates = './data_simulator/huge_dataset/gencand/'\n",
    "\n",
    "with open(path_candidates + \"dict_flattened_candidates.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "### Load the dictory containing the flattened objects ID lists associated with the candidates ###\n",
    "flat_ids, indptr, lenghts = data['flat_ids'], data['start_pos'], data['lengths']\n",
    "del data\n",
    "\n",
    "# Ensure the big arrays are contiguous (helps memmap efficiency)\n",
    "flat_ids = np.ascontiguousarray(flat_ids)\n",
    "indptr   = np.ascontiguousarray(indptr)\n",
    "lenghts  = np.ascontiguousarray(lenghts)\n",
    "\n",
    "# Check that there is no candidate with 0 associated objects. It shouldn't happen, but we do a quick sanity check here.\n",
    "assert np.all(lenghts == np.diff(indptr)), \"lenghts/indptr mismatch (or empty segments present)\"\n",
    "assert np.all(lenghts > 0), \"Candidates with zero associated objects detected, should not happen!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the dataset with the objects' \"true\" labels.\n",
    "\n",
    "**TODO**: we are using dummy labels for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the dataset containing the true labels of the objects.\n",
    "n_objects = 100000\n",
    "positive_rate = 0.6\n",
    "labels = np.random.default_rng(42).binomial(n=1, p=positive_rate, size=n_objects).astype(np.int8)\n",
    "# labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the empirical distribution of the considered test statistic with a certain number of Monte Carlo simulations.\n",
    "\n",
    "Here we perform the Monte Carlo simulations needed to determine the distribution of the test statistics under the assumption that the null hypothesis is true. The test statistics used is the maximum log likelihood ratio computed across the regions of all the grids, while the likelihood function is the Bernoulli-based one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "\n",
    "# Segmented reduction function used within 'batch_max_likelihood_ratio'\n",
    "@nb.njit(cache=True, nogil=True, fastmath=True)\n",
    "def segmented_sum_indirect(labels_objects, flat_ids, indptr):\n",
    "    n = indptr.size - 1\n",
    "    out = np.empty(n, dtype=np.uint32)\n",
    "    for i in range(n):\n",
    "        s = 0\n",
    "        start = indptr[i]\n",
    "        end   = indptr[i + 1]\n",
    "        for p in range(start, end):\n",
    "            s += labels_objects[flat_ids[p]]\n",
    "        out[i] = s\n",
    "    return out\n",
    "\n",
    "def batch_max_likelihood_ratio(labels_objects: np.ndarray, \n",
    "                               flat_ids: np.ndarray, indptr: np.ndarray, lenghts: np.ndarray,\n",
    "                               tot_sum_labels: int,\n",
    "                               logL0_max: float) -> tuple[np.ndarray, np.ndarray, np.ndarray, float]:    \n",
    "\n",
    "    # Gather labels for all ids, then sum per candidate via segmented reduction.\n",
    "    #\n",
    "    # NOTE: this version instantiates an output array of the same size of 'flat_ids', so it can be large!\n",
    "    # flat_vals = labels_objects[flat_ids]\n",
    "    # inside_sum = np.add.reduceat(flat_vals, indptr[:-1]).astype(np.uint32, copy=False)\n",
    "    #\n",
    "    # NOTE: this version computes the segmented reduction on the fly. It is faster and avoids allocating\n",
    "    #       large 'flat_vals' arrays.\n",
    "    inside_sum = segmented_sum_indirect(labels_objects, flat_ids, indptr)\n",
    "\n",
    "    # Vectorized computation: for each candidate subset of cells, compute the positive rate of the objects\n",
    "    # associated with the subset vs the positive rate of the other objects.\n",
    "    # NOTE: we use np.divide with the `where` parameter to avoid divisions by zero.\n",
    "    p, n = inside_sum, lenghts\n",
    "    P, N = tot_sum_labels, labels_objects.size\n",
    "    inside_positive_rate  = np.divide(p, n, out=np.zeros_like(p, dtype=np.float32), where=(n > 0))\n",
    "    outside_positive_rate = np.divide(P - p, N - n, out=np.zeros_like(p, dtype=np.float32), where=((N - n) > 0))\n",
    "    \n",
    "\n",
    "    # Potentially numpy-unsafe computation of the log-likelihood under the alternative hypotesis.\n",
    "    #logL1 = (p * np.log(inside_positive_rate)\n",
    "    #         + (n - p) * np.log1p(-inside_positive_rate)\n",
    "    #         + (P - p) * np.log(outside_positive_rate)\n",
    "    #         + (N - n - (P - p)) * np.log1p(-outside_positive_rate))\n",
    "    \n",
    "    \n",
    "    # Safe alternative computation of the log-L1 via scipy functions. \n",
    "    # NOTE: the log-likelihood is -inf when the positive rate is 0 or 1, which can happen when p==0 or p==n for the inside positive rate, \n",
    "    # or when P-p==0 or N-n-(P-p)==0 for the outside positive rate. This is not a problem per se, since we are interested in the likelihood\n",
    "    # ratio, and if the likelihood under the alternative hypotesis is -inf, then the likelihood ratio will be 0, which is what we expect in\n",
    "    # these cases.\n",
    "    # valid = (n > 0) & (n < N) # optional: mask degenerate windows (n==0 or n==N)\n",
    "    # logL1 = np.full_like(inside_positive_rate, -np.inf, dtype=np.float32)\n",
    "    logL1 = ( xlogy(p, inside_positive_rate) + \n",
    "              xlog1py((n - p), -inside_positive_rate) +\n",
    "              xlogy((P - p), outside_positive_rate) +\n",
    "              xlog1py((N - n - (P - p)), -outside_positive_rate) )\n",
    "\n",
    "    # Vectorized computation of the log-likelihood ratio of the candidates\n",
    "    # logLR = logL1 - logL0_max\n",
    "    logL1 -= logL0_max\n",
    "    maxLogLR = float(np.nanmax(logL1))\n",
    "\n",
    "    return inside_positive_rate, outside_positive_rate, logL1, maxLogLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import numba as nb\n",
    "\n",
    "@nb.njit(cache=True, nogil=True)\n",
    "def max_loglr_streaming(is_simulation,\n",
    "                        labels, \n",
    "                        flat_ids, indptr, lengths, \n",
    "                        P, logL0_max):\n",
    "    N = labels.size\n",
    "    max_lr = -np.inf\n",
    "    m = lengths.size\n",
    "    lr_candidates = np.empty(m if not is_simulation else 0)\n",
    "    in_rate_candidates = np.empty(m if not is_simulation else 0)\n",
    "    out_rate_candidates = np.empty(m if not is_simulation else 0)\n",
    "    for i in range(m):\n",
    "        start = indptr[i]\n",
    "        end   = indptr[i + 1]\n",
    "\n",
    "        # segmented sum without allocating flat_vals\n",
    "        p = 0\n",
    "        for k in range(start, end):\n",
    "            p += labels[flat_ids[k]]\n",
    "\n",
    "        n = lengths[i]\n",
    "        if (n <= 0) or (n >= N) : continue  # degenerate candidate (no object or contains all objects)\n",
    "\n",
    "        # Positive rate for the objects associated with the candidate, and for the objects that are not.\n",
    "        inside_rate  = p / n\n",
    "        outside_rate = (P - p) / (N - n)\n",
    "\n",
    "        # compute logL1 safely (xlogy/xlog1py equivalents)\n",
    "        logL1 = p * math.log(inside_rate) if p > 0 else 0.0\n",
    "        logL1 += (n - p) * math.log1p(-inside_rate) if n - p > 0 else 0.0\n",
    "        #\n",
    "        Pout = P - p\n",
    "        Nout = N - n\n",
    "        logL1 += Pout * math.log(outside_rate) if Pout > 0 else 0.0\n",
    "        #\n",
    "        neg_out = Nout - Pout\n",
    "        logL1 += neg_out * math.log1p(-outside_rate) if neg_out > 0 else 0.0\n",
    "\n",
    "        # Compute the log-likelihood ratio.\n",
    "        lr = logL1 - logL0_max\n",
    "        \n",
    "        # Add the ratio to the vector of log-LRs of the candidates.\n",
    "        if (not is_simulation) : \n",
    "            lr_candidates[i] = lr\n",
    "            in_rate_candidates[i], out_rate_candidates[i] = inside_rate, outside_rate\n",
    "\n",
    "        # Update the candidate with the largest log-LR found so far.\n",
    "        if lr > max_lr:\n",
    "            max_lr = lr\n",
    "\n",
    "    return in_rate_candidates, out_rate_candidates, lr_candidates, max_lr\n",
    "\n",
    "\n",
    "@nb.njit(cache=True, nogil=True, parallel=True)\n",
    "def compute_simulations(num_sims, labels, \n",
    "                        flat_ids, indptr, lengths,\n",
    "                        P, logL0_max):\n",
    "    \n",
    "    vec_max_LR = np.empty(num_sims, dtype=np.float32)\n",
    "    for s in nb.prange(num_sims) :\n",
    "        # Shuffle the labels.\n",
    "        np.random.seed(s)\n",
    "        shuffled_labels = labels.copy()\n",
    "        np.random.shuffle(shuffled_labels)\n",
    "\n",
    "        # Compute the max log-LR distribution expected under the assumption that H_0 is true.\n",
    "        vec_max_LR[s] = max_loglr_streaming(True, shuffled_labels, flat_ids, indptr, lengths, P, logL0_max)[-1]\n",
    "\n",
    "    return vec_max_LR"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### PARALLEL JOBLIB VERSION ###\n",
    "\n",
    "### JOBLIB HELPER FUNCTION ###\n",
    "\n",
    "def one_simulation(i, labels, flat_ids, indptr, lenghts, P, logL0_max):\n",
    "    rng = np.random.default_rng(i)\n",
    "    shuffled_labels = rng.permutation(labels)\n",
    "    # return batch_max_likelihood_ratio(shuffled_labels, flat_ids, indptr, lenghts, P, logL0_max)[3]\n",
    "    return max_loglr_streaming(shuffled_labels, flat_ids, indptr, lenghts, P, logL0_max)[1]\n",
    "\n",
    "### JOBLIB MAIN CODE ###\n",
    "\n",
    "# Compute the L_0 likelihood, which models the likelihood of observing the labels in the data under the the assumption that the \n",
    "# null hypotesis H_0 is true, i.e., there is a single global distribution that governs the labels. L_0 is constant across \n",
    "# permutations, since it depends only on the total number of positive and negative labels in the dataset, which does not change\n",
    "# when shuffling the original labels.\n",
    "P = labels.sum(dtype=np.uint32) # Overall number of objects with positive labels.\n",
    "N = labels.size                 # Overall number of objects.\n",
    "rho = P / N\n",
    "logL0_max = P * np.log(rho) + (N - P) * np.log1p(-rho)\n",
    "\n",
    "# Compute the simulations' max log-LRs in parallel.\n",
    "vec_max_LR = Parallel(n_jobs=-1,\n",
    "                      backend=\"loky\",\n",
    "                      verbose=10,\n",
    "                      max_nbytes=\"1M\",   # threshold that triggers auto-memmapping\n",
    "                      mmap_mode=\"r\")(delayed(one_simulation)(i, labels, flat_ids, indptr, lenghts, P, logL0_max) for i in range(num_simulations))\n",
    "vec_max_LR = np.asarray(vec_max_LR, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### SEQUENTIAL VERSION ###\n",
    "\n",
    "# Compute the L_0 likelihood, which models the likelihood of observing the labels in the data under the the assumption that the \n",
    "# null hypotesis H_0 is true, i.e., there is a single global distribution that governs the labels. L_0 is constant across \n",
    "# permutations, since it depends only on the total number of positive and negative labels in the dataset, which does not change\n",
    "# when shuffling the original labels.\n",
    "P = labels.sum(dtype=np.uint32) # Constant across permutations\n",
    "N = labels.size\n",
    "rho = P / N\n",
    "logL0_max = P * np.log(rho) + (N - P) * np.log1p(-rho)\n",
    "\n",
    "\n",
    "vec_max_LR = np.empty(num_simulations, dtype=np.float32)\n",
    "for i in tqdm(range(num_simulations)):    \n",
    "    # Shuffle the original labels assigned to the objects. This represents the null hypotesis H_0, according to which\n",
    "    # there is a single global distribution that governs the labels, i.e., there is not one or more sets of geographical regions\n",
    "    # in which the associated objects have an average positive rate that is significantly different than that of the other objects. \n",
    "    rng = np.random.default_rng(i)\n",
    "    shuffled_labels = rng.permutation(labels)\n",
    "\n",
    "    # For the objects associated with each subset of cells, compute their positive rate vs that of the other objects.\n",
    "    # _, _, _, vec_max_LR[i] = batch_max_likelihood_ratio(shuffled_labels,\n",
    "    #                                                    flat_ids, indptr, lenghts,\n",
    "    #                                                    P, logL0_max)\n",
    "    _, vec_max_LR[i] = max_loglr_streaming(shuffled_labels, flat_ids, indptr, lenghts, P, logL0_max)\n",
    "\n",
    "\n",
    "# DEBUG\n",
    "# np_list_candidates_inrate, np_list_candidates_outrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PARALLEL NUMBA VERSION ###\n",
    "\n",
    "# Compute the L_0 likelihood, which models the likelihood of observing the labels in the data under the the assumption that the \n",
    "# null hypotesis H_0 is true, i.e., there is a single global distribution that governs the labels. L_0 is constant across \n",
    "# permutations, since it depends only on the total number of positive and negative labels in the dataset, which does not change\n",
    "# when shuffling the original labels.\n",
    "P = labels.sum(dtype=np.uint32) # Constant across permutations\n",
    "N = labels.size\n",
    "rho = P / N\n",
    "logL0_max = P * np.log(rho) + (N - P) * np.log1p(-rho)\n",
    "\n",
    "# Compute the simulations' max log-LRs in parallel via numba.\n",
    "vec_max_LR = compute_simulations(num_simulations, labels, \n",
    "                                 flat_ids, indptr, lenghts, \n",
    "                                 P, logL0_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sort the max_LR distribution obtained with the simulations.\n",
    "# This is the distribution of the likelihood ratios of the most extreme regions observed empirically\n",
    "# under the assumption that the null hypothesis H_0 is true.\n",
    "sorted_vec_max_LR = np.sort(vec_max_LR)\n",
    "\n",
    "# Print the most extreme likelihood ratio observed across all the simulations.\n",
    "print(sorted_vec_max_LR[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the max log likelihood ratio from the candidates when considering the original labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inside_positive_rate, outside_positive_rate, vec_LR_dataset, max_LR_dataset = \\\n",
    "#    batch_max_likelihood_ratio(labels,\n",
    "#                               flat_ids, indptr, lenghts,\n",
    "#                               P, logL0_max)\n",
    "\n",
    "inside_positive_rate, outside_positive_rate, vec_LR_dataset, max_LR_dataset = \\\n",
    "    max_loglr_streaming(False, labels, flat_ids, indptr, lenghts, P, logL0_max)\n",
    "\n",
    "# DEBUG\n",
    "inside_positive_rate, outside_positive_rate, vec_LR_dataset, max_LR_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine if $H_0$ must be rejected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine where the max LR computed with the original labels fall in the empirical test statistic's distribution.\n",
    "rank = np.count_nonzero(vec_max_LR >= max_LR_dataset)\n",
    "\n",
    "# Monte Carlo p-value of the observed test statistic's value derived from the ranked empirical test statistic's distribution \n",
    "# (right tail), with +1 correction to include max_LR_dataset itself.\n",
    "p_value = (rank + 1) / (num_simulations + 1)\n",
    "\n",
    "# Based on the distribution and the real data we have, decide if we have to reject H_0.\n",
    "reject_H0 = p_value <= alpha\n",
    "\n",
    "print(f\"Statistical significance alpha: {alpha}\")\n",
    "print(f\"Position in sorted MC sample: {rank}/{num_simulations} (extreme if below position {int(num_simulations * alpha)})\")\n",
    "print(f\"Monte Carlo p-value: {p_value:.6f}\")\n",
    "print(f\"Decision: {'Reject H0' if reject_H0 else 'Do NOT reject H0'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Various DEBUGS and SANITY CHECKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG and SANITY CHECK: plot the distribution of the maximum likelihood ratios computed in the Monte Carlo simulations.\n",
    "#\n",
    "# Lots of simulations produce “modest” max log-LR, and a few simulations produce unusually large max LLRs.\n",
    "# So, we expect to see a concentration of max LLRs on the left, with a long right tail.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(vec_max_LR, bins=50)\n",
    "plt.xlabel(\"Values of the simulations' max likelihood ratios\")\n",
    "plt.ylabel(f\"Frequency (log-scale) (total: {vec_max_LR.size} simulations)\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: plot the distribution of the likelihood ratios computed over the subsets of cells when considering the original labels.\n",
    "plt.hist(vec_LR_dataset, bins=200)  # tune bins (100–500 usually fine)\n",
    "plt.xlabel(\"Values of the candidates' likelihood ratios\")\n",
    "plt.ylabel(f\"Frequency (log-scale) (total: {vec_LR_dataset.size} objects)\")\n",
    "plt.yscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: find the characteristics of the subset of cells with the maximum LR when considering the original labels.\n",
    "\n",
    "# Find where the candidate with the max LR is located.\n",
    "idx = np.argsort(vec_LR_dataset)\n",
    "\n",
    "# Sort the 1D arrays of interest accordingly.\n",
    "vec_LR_dataset_sorted = vec_LR_dataset[idx]\n",
    "lens_sorted = lenghts[idx]\n",
    "inside_positive_rate_sorted = inside_positive_rate[idx]\n",
    "outside_positive_rate_sorted = outside_positive_rate[idx]\n",
    "\n",
    "# Print the max LR candidate's info.\n",
    "print(f\"Info most 'problematic' candidate: local ps: {inside_positive_rate_sorted[-1]}, \" +\n",
    "      f\"other ps: {outside_positive_rate_sorted[-1]}, num_objs_candidate: {lens_sorted[-1]}, \" +\n",
    "      f\"likelihood ratio: {vec_LR_dataset_sorted[-1]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
