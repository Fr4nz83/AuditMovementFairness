{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing a Trajectory Classifier for Spatial Fairness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import folium\n",
    "\n",
    "from src.functions import *\n",
    "from src.traj_functions import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the output of a trajectory classifier\n",
    "\n",
    "Here we load a dataset that should contain the labels given by a classifier to trajectories. In the dataset, we expect to find:\n",
    "\n",
    "1. the labels given to each trajectory;\n",
    "2. the sequence of points associated with each trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the labelled trajectory dataset.\n",
    "dataset_path = './synth_traj_dataset_unfair.pkl'\n",
    "gdf_trajs = gpd.GeoDataFrame(pd.read_pickle(dataset_path))\n",
    "display(gdf_trajs.head())\n",
    "\n",
    "# Set the name of the column containing the labels given to the trajectories.\n",
    "label = 'label'\n",
    "N, P = get_stats(gdf_trajs, label)\n",
    "\n",
    "# Function below remaps labels different than \"1\" to \"0\". Used to reduce multi-class classification to binary classification.\n",
    "true_types = get_true_types(gdf_trajs, label)\n",
    "# print(true_types[:30])\n",
    "\n",
    "# Print some general statistics about the points and labels. Here, the positive label is \"1\", the other ones are considered negative.\n",
    "print(f'N={N} points, P={P} positives, frac.positives={P/N:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lat_max = df['lat'].values.max()\n",
    "# lat_min = df['lat'].values.min()\n",
    "# lon_max = df['lon'].values.max()\n",
    "# lon_min = df['lon'].values.min()\n",
    "\n",
    "# mapit = folium.Map(location=[37.09, -95.71], zoom_start=5, tiles=\"Stamen Toner\")\n",
    "\n",
    "# for index, row in df.iterrows():\n",
    "#     if row[label] == 1:\n",
    "#         folium.CircleMarker( location=(row['lat'], row['lon']), color='#00FF00', fill_color='#00FF00', fill=True, opacity=0.4, fill_opacity=0.4, radius=2 ).add_to( mapit )\n",
    "#     elif row[label] == 0:\n",
    "#         folium.CircleMarker( location=(row['lat'], row['lon']), color='#FF0000', fill_color='#FF0000', fill=True, opacity=0.4, fill_opacity=0.4, radius=2 ).add_to( mapit )\n",
    "\n",
    "\n",
    "# mapit.fit_bounds([(lat_min, lon_min), (lat_max, lon_max)])\n",
    "\n",
    "# mapit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Run Experiments\n",
    "\n",
    "There are three experiments:\n",
    "- (TODO) Unrestricted regions: runs **our approach** on unrestricted regions. Unrestricted regions should be somehow computed from the clusters of a trajectory clustering algorithm.\n",
    "- Regions are cells of a uniform grid: evaluates **our approach** over a single partitioning.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Partition Auditing\n",
    "\n",
    "Instead of creating regions from clusters of trajectories/points, here we use a more common approach.\n",
    "We superimpose a uniform grid over the space: the grid's cells will be the regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create the partitioning (grid) and its partitions (regions)\n",
    "num_slices_lon = 20\n",
    "num_slices_lat = 20\n",
    "grid_info, grid_loc2_idx, regions = create_traj_partitioning(gdf_trajs, num_slices_lon, num_slices_lat)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audit the classifier's output with our approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the test statistics for each region with the real data.\n",
    "best_region, max_likeli, statistics = scan_regions(regions, true_types, N, P, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conduct 'n_alt_worlds' simulations. \n",
    "# In each simulation, we shuffle the labels of the trajectories and compute the test statistics for each region. \n",
    "# We then consider the distribution of the test statistics obtained in the simulations to determine the significance threshold.\n",
    "# The significance threshold is determined using 'signif_level', i.e., the desired quantile over the distribution of the test\n",
    "# statistics obtained in the simulations.\n",
    "#\n",
    "# Example: if we conduct 200 simulations, and we have a signif_level of 0.005, then we are interested in\n",
    "#          regions whose test statistics is larger than the top 200*0.005 = 1st test statistics obtained\n",
    "#          in the simulations.\n",
    "n_alt_worlds = 200\n",
    "signif_level = 0.005\n",
    "signif_thresh = get_signif_threshold(signif_level, n_alt_worlds, regions, N, P)\n",
    "print(signif_thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Number of regions to evaluate: {len(statistics)}')\n",
    "\n",
    "### Identify regions with test statistic above the statistical significance threshold ###\n",
    "\n",
    "# Sort 'statistics' in ascending order.\n",
    "sorted_statistics = np.sort(statistics)\n",
    "\n",
    "# 'np.searchsorted' finds the index in the sorted array where 'signif_thresh' \n",
    "# should be inserted to keep the array sorted. This effectively counts how many\n",
    "# values are smaller than signif_thresh. Then, subtract this index from len(statistics):\n",
    "# this finds how many values are greater than signif_thresh.\n",
    "top_k = len(statistics) - np.searchsorted(sorted_statistics, signif_thresh)\n",
    "\n",
    "# 'np.argsort(statistics)' returns the indices that would sort the statistics array in ascending order.\n",
    "# '[::-1]' reverses the array, so that the highest values come first.\n",
    "# '[:top_k]' selects the indexes of the top_k regions with the test statistics larger than signif_thresh: \n",
    "# these represent the 'unfair' regions.\n",
    "desc_sorted_indexes = np.argsort(statistics)[::-1]\n",
    "indexes_unfair_regions = desc_sorted_indexes[:top_k]\n",
    "indexes_fair_regions = desc_sorted_indexes[top_k:]\n",
    "\n",
    "significant_regions = [regions[i] for i in indexes_unfair_regions]\n",
    "normal_regions = [regions[i] for i in indexes_fair_regions]\n",
    "\n",
    "print(f'Indexes of unfair regions: {indexes_unfair_regions}')\n",
    "print(f'{len(significant_regions)} significant (unfair) regions found')\n",
    "print(f'{len(normal_regions)} fair regions found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display on a map the grid and the regions with the test statistics above the significance threshold.\n",
    "show_traj_grid_regions(gdf_trajs, grid_info, true_types, normal_regions, significant_regions, label_trajs='label')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
